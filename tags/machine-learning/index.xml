<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning | Rene Bidart</title>
    <link>https://renebidart.com/tags/machine-learning/</link>
      <atom:link href="https://renebidart.com/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>machine learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 04 Dec 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>machine learning</title>
      <link>https://renebidart.com/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>3D Pose Estimation Datasets</title>
      <link>https://renebidart.com/post/2018-12-04-pose-datasets/2018-12-04-pose/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2018-12-04-pose-datasets/2018-12-04-pose/</guid>
      <description>&lt;p&gt;2D pose estimation has improved immensely over the past few years, partly because of wealth of data stemming from the ease of annotating any RGB video. 3D pose annotation is much more difficult because accurate 3D pose annotation requires using motion capture in indoor artificial settings.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../pose/mocap.jpg&#34; alt=&#34;mocap.jpg&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Mocap requires artificial settings and unnatural clothing, as shown in the 
&lt;a href=&#34;https://ps.is.tuebingen.mpg.de/pages/motion-capture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Max Plank&lt;/a&gt; lab&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ground truth vicon datasets can be augmented with synthesized poses using different body shapes, background, etc. but still are limited by the original collected data. Alternatively, synthetic datasets can be used to increase variety. In addition, some use RGB cameras and auxiliary measurement to estimate 3D poses in the wild.&lt;/p&gt;
&lt;p&gt;Some promising datasets are 
&lt;a href=&#34;http://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3D Poses in the Wild&lt;/a&gt; for in the wild poses, 
&lt;a href=&#34;https://arxiv.org/pdf/1803.08319.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JTA&lt;/a&gt; if for simulated data, and 
&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6682899&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human3.6m&lt;/a&gt; and 
&lt;a href=&#34;https://cvssp.org/projects/totalcapture/TotalCapture/TrumbleBMVC2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Total Capture&lt;/a&gt; for controlled environments with ground truth labels.&lt;/p&gt;
&lt;h2 id=&#34;ground-truth-datasets&#34;&gt;Ground Truth Datasets&lt;/h2&gt;
&lt;p&gt;These datasets include ground truth, including full 3d skeleton using mocap, which has negligible measurement error. The limitation is mocap tends to only work in controlled environments, so models trained on these datasets may struggle to generalize.&lt;/p&gt;
&lt;p&gt;Alternatively, synthetic data can be generated either based on human data, as in SURREAL, or entirely synthetically using a game engine as in JTA. Both allow for good quality of annotations, but are limited because of the computer rendered graphics rather than true natural images.&lt;/p&gt;
&lt;h3 id=&#34;human36m-large-scale-datasets-and-predictive-methods-for-3d-human-sensing-in-natural-environments-2014&#34;&gt;Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments (2014)&lt;/h3&gt;
&lt;p&gt;This is the standard in 3d pose estimation. A dataset of 11 people doing 17 common poses in an indoor environment, resulting in a total of 3.6 million frames. The following measurements are included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RGB views: 4 standard ones, one with depth&lt;/li&gt;
&lt;li&gt;Exact 3D joint positions from mocap&lt;/li&gt;
&lt;li&gt;Pixel-level body part labels for a subset of the data&lt;/li&gt;
&lt;li&gt;3D laser scans of the actors (done once, not throughout the videos)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This dataset mostly includes everyday poses, not anything sports specific. natural scenarios, but no specific sports, etc.&lt;/p&gt;
&lt;p&gt;They include some mixed reality augmentation for the dataset as well. Details can be found in 
&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6682899&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../36m.png&#34; alt=&#34;36m.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Example poses from Human3.6m&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;humaneva-2009&#34;&gt;HumanEva (2009)&lt;/h3&gt;
&lt;p&gt;Similar to the 3.6m dataset, but more limited. It is actually composed of two datasets, HumanEva I &amp;amp; HumanEva II, both in controlled indoor environments.&lt;/p&gt;
&lt;p&gt;The HumanEva I dataset consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RGB views: 3 standard ones&lt;/li&gt;
&lt;li&gt;60 Hz 3D joint positions from mocap&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HumanEva II is similar, but with the addition of another camera, as well as multi action scenarios including running around a loop performing actions.&lt;/p&gt;
&lt;p&gt;These datasets have more natural clothes, but this comes with the cost of moving sensors, and less accurate mocap data. Details can be found in 
&lt;a href=&#34;http://humaneva.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HumanEva&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../humaneva.png&#34; alt=&#34;humaneva.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Examples from HumanEva II&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;total-capture-3d-human-pose-estimation-fusing-video-and-inertial-sensors&#34;&gt;Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors&lt;/h3&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://cvssp.org/projects/totalcapture/TotalCapture/TrumbleBMVC2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Total Capture&lt;/a&gt; dataset contains indoor multi-view video, IMU, and vicon mocap with âˆ¼1.9M frames. Very constrained environment.&lt;/p&gt;
&lt;p&gt;This dataset is useful to compare 3d pose estimating using IMU and multiview cameras to ground truth using mocap, but again is quite limited by being in a controlled environment.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../total.png&#34; alt=&#34;total.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Different data types contained in Total Capture&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;learning-from-synthetic-humans-2017&#34;&gt;Learning from Synthetic Humans (2017)&lt;/h3&gt;
&lt;p&gt;SURREAL (Synthetic hUmans foR REAL tasks) dataset contains synthetically generated data rendered from 3D sequences of motion capture data. The original data is from Human3.6M.&lt;/p&gt;
&lt;p&gt;The idea is that you can randomly sample the person&amp;rsquo;s pose, appearance, lighting, camera position and background, creating a massive dataset from only a few annotated models. The SMPL model is used to decompose the body into pose and shape parameters, so we can sample these independently to produce an image. To generate realistic shapes, they use the CAESAR dataset to train SMPL, and then randomly select a participant and perturb the shape components slightly.&lt;/p&gt;
&lt;p&gt;Once the person is generated from the random shape and pose parameters, sample camera, texture, light and background to get the full scene. Details can be found in 
&lt;a href=&#34;https://arxiv.org/abs/1701.01370&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning from Synthetic Humans&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../surreal.png&#34; alt=&#34;surreal.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Surreal pipeline for generating data&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;jta-dataset-2018&#34;&gt;JTA Dataset (2018)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../gta5.png&#34; alt=&#34;gta5.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Surreal pipeline for generating data&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Massive simulated dataset created from Grand Theft Auto V. Consists of about 500 000 frames with almost 10 million body poses labeled with full 3D annotation.&lt;/p&gt;
&lt;p&gt;Details can be found in 
&lt;a href=&#34;https://arxiv.org/pdf/1803.08319.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;in-the-wild&#34;&gt;In the wild&lt;/h2&gt;
&lt;p&gt;Instead of using ground truth mocap annotations, annotations are estimated in order to get a wider variety of actions in the wild instead of being constrained to indoor setting with mocap. Ground truth can be estimated using auxiliary measurements such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inertial measurement units&lt;/li&gt;
&lt;li&gt;Multiview cameras&lt;/li&gt;
&lt;li&gt;RGBD cameras&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3d-poses-in-the-wild-dataset&#34;&gt;3D Poses in the Wild Dataset&lt;/h3&gt;
&lt;p&gt;60 in the wild videos with 2D and 3D pose annotation, including camera pose and scanned models with different clothing variations. The poses are estimated using IMUs and a handheld 2d video. This is the only promising 3d pose in the wild dataset.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../wild.png&#34; alt=&#34;wild.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Actual in the wild images with 3d poses&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On TotalCapture dataset this gets 26mm error, and outperforms their method without using multiview. This is a best case scenario for accuracy because it was in a controlled environment, so we should expect this dataset to have error &amp;gt;26mm. Nevertheless, this could be very useful for situations where absolute accuracy isn&amp;rsquo;t crucial, and we want to generalize to the wild. More details on their 
&lt;a href=&#34;http://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;monocular-3d-human-pose-estimation-in-the-wild-using-improved-cnn-supervision-2016&#34;&gt;Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision (2016)&lt;/h3&gt;
&lt;p&gt;Using multi-camera markerless system, recording 3D pose in green screen to change background.  8 actors, 8 cameras, &amp;gt;1.3M frames. This is still indoor in a relatively controlled environment, so it isn&amp;rsquo;t truly in the wild, and there may be some uncertainty with how accurate their ground truth is. More info in their 
&lt;a href=&#34;https://arxiv.org/pdf/1611.09813.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;unite-the-people-closing-the-loop-between-3d-and-2d-human-representations-2017&#34;&gt;Unite the People: Closing the Loop Between 3D and 2D Human Representations (2017)&lt;/h3&gt;
&lt;p&gt;In 
&lt;a href=&#34;https://arxiv.org/abs/1701.02468&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unite the People&lt;/a&gt; they use RGB to 3D estimation based on CNN, with some manual removal of bad results, and no uncertainty estimate. They tested their model on model on Human3.6m, getting 80mm average error. This is probably best case error, so this isn&amp;rsquo;t useful for most applications.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
