<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Rene Bidart">

  
  
  
    
  
  <meta name="description" content="Attention In the past the standard design of NLP models, was based on recurrent neural networks (RNN) to ensure the model can encode long range dependencies necessary in language modelling. This assumption was called into question with the development of the Transformer, a model where instead of RNNs, self-attention layers are used.">

  
  <link rel="alternate" hreflang="en-us" href="https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157504369-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-157504369-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu5e15e861ad43a166b29f0ae919c910d4_1675_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu5e15e861ad43a166b29f0ae919c910d4_1675_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Rene Bidart">
  <meta property="og:url" content="https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/">
  <meta property="og:title" content="Attention is Overrated | Rene Bidart">
  <meta property="og:description" content="Attention In the past the standard design of NLP models, was based on recurrent neural networks (RNN) to ensure the model can encode long range dependencies necessary in language modelling. This assumption was called into question with the development of the Transformer, a model where instead of RNNs, self-attention layers are used."><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-04-26T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-04-26T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/"
  },
  "headline": "Attention is Overrated",
  
  "datePublished": "2019-04-26T00:00:00Z",
  "dateModified": "2019-04-26T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Rene Bidart"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Rene Bidart",
    "logo": {
      "@type": "ImageObject",
      "url": "https://renebidart.com/images/icon_hu5e15e861ad43a166b29f0ae919c910d4_1675_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Attention In the past the standard design of NLP models, was based on recurrent neural networks (RNN) to ensure the model can encode long range dependencies necessary in language modelling. This assumption was called into question with the development of the Transformer, a model where instead of RNNs, self-attention layers are used."
}
</script>

  

  


  


  





  <title>Attention is Overrated | Rene Bidart</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Rene Bidart</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Rene Bidart</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Attention is Overrated</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 26, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="attention">Attention</h2>
<p>In the past the standard design of NLP models, was based on recurrent neural networks (RNN)  to ensure the model can encode long range dependencies necessary in language modelling. This assumption was called into question with the development of the 
<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Transformer</a>, a model where instead of RNNs, self-attention layers are used. The transformer is composed of an encoder and decoder stack of alternating pointwise fully-connected and self-attention layers:</p>
<table>
<thead>
<tr>
<th><img src="../transformer.png" alt="transformer.png"></th>
</tr>
</thead>
<tbody>
<tr>
<td>:&ndash;:</td>
</tr>
<tr>
<td><em>The Transformer. If this isn&rsquo;t familiar check out the 
<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">paper</a> or this great 
<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">blog</a></em></td>
</tr>
</tbody>
</table>
<p>Since then self-attention has been useful in a variety of fields, including computer vision, achieving state of the art in 
<a href="https://arxiv.org/abs/1812.03411" target="_blank" rel="noopener">adversarial defence on imagenet</a>, as well as in the 
<a href="https://arxiv.org/abs/1805.08318" target="_blank" rel="noopener">Self-Attention GAN</a>.</p>
<p>The transformer model made us question the standard design of NLP models being focused on recurrent neural networks, but even more recently the idea that attention is the key for NLP has been questioned.</p>
<h2 id="limitations-of-attention">Limitations of Attention</h2>
<p>The 
<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT paper</a> showed us that attention isn&rsquo;t all you need to achieve good results in NLP, because to achieve their state of the art results they leveraged bidirectional LSTMs in addition to the self attention used in the transformer. But later work has shown not only do we need more than attention, we may not need attention at all. First we can review some limitations of attention:</p>
<h3 id="quadratic-complexity">Quadratic Complexity</h3>
<p>Attention has a quadratic complexity in input length, meaning attention doesn&rsquo;t scale well over long distances.</p>
<table>
<thead>
<tr>
<th align="center"><img src="../att-complexity.png" alt="att-complexity.png"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><em>From the 
<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Transformer</a> paper</em></td>
</tr>
</tbody>
</table>
<p>This is because for each position (there are n), we need to attend to every other position in the input (there are also n), for a total of d dimensions in each location, so in total there are $$n^2d$$ operations needed, where d is the dimension of the input.</p>
<h3 id="limitations-of-attention-for-long-range-dependencies">Limitations of Attention for Long Range Dependencies</h3>
<p>People had speculated that the reason attention was so useful was because it could more easily model long range dependencies in the input, but a recent 
<a href="https://arxiv.org/pdf/1808.08946.pdf" target="_blank" rel="noopener">paper</a> has shown this to be incorrect.</p>
<p>By looking at the performance of CNN, RNN and the transformer on on subject-verb agreement task as the distance between the subject and verb is increased, they showed that self-attention&rsquo;s performance degrades faster than CNNs or RNNs. To overcome this limitation in performance for long range dependencies, many self attention heads are needed.</p>
<table>
<thead>
<tr>
<th align="center"><img src="../att-long-rang.png" alt="att-long-rang.png"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">*Accuracy on subject-verb agreement task. Attention is worse than CNNs and RNNs! From</td>
</tr>
<tr>
<td align="center">
<a href="https://arxiv.org/pdf/1808.08946.pdf" target="_blank" rel="noopener">A Targeted Evaluation of Neural Machine Translation Architectures</a>*</td>
</tr>
</tbody>
</table>
<h3 id="the-alternative-lightweight-dynamic-convolutions">The Alternative: Lightweight, Dynamic Convolutions</h3>
<p>In 
<a href="https://openreview.net/pdf?id=SkVhlh09tX" target="_blank" rel="noopener">Pay Less Attention with Lightweight and Dynamic Convolutions</a>, they propose an alternative to self in the form of a two modified convolution layers:</p>
<ol>
<li><strong>Lightweight Convolutions</strong> - Depthwise separable convolutions with weight sharing</li>
<li><strong>Dynamic Convolutions</strong> - An extension of lightweight convolutions with dynamic weights</li>
</ol>
<h4 id="lightweight-convolutions">Lightweight Convolutions</h4>
<p>Lightweight convolutions leverage an innovation that is commonly used in vision to create more efficient architectures, known as Depthwise Convolutions.</p>
<p><strong>Depthwise Convolutions</strong> take one channel as input for each convolution,  in contrast to the standard convolution, which takes all channels as input . Only taking a subset of channels as input massively reduces the number of parameters, and as was seen in the vision literature still provides good performance.</p>
<table>
<thead>
<tr>
<th align="center"><img src="../depthwise.png" alt="depthwise.png"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><em>As shown in 
<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">MobileNet</a>, depthwise convolutions apply a single filter for each channel.</em></td>
</tr>
</tbody>
</table>
<p><strong>Weight Sharing</strong> across channels is also added to the depthwise convolution to further reduce the number of parameters. They use the same weights across multiple channels, for a total of H independent weights. In the paper they used a value of $$H=16$$, so instead of learning 1024 different filters, only 16 are learned, in the case that there are 1024 channels.</p>
<p>Using the Depthwise convolution reduces the parameters required from $$d^2k$$ to $$dk$$, where d is the number of channels, and k is filter width. The addition of weight sharing further reduces the number from $$dk$$ to $$Hk$$.</p>
<table>
<thead>
<tr>
<th align="center"><img src="../compare-conv.png" alt="compare-conv.png"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><em>Comparison of self attention, Lightweight convolution and dynamic convolution from 
<a href="https://openreview.net/pdf?id=SkVhlh09tX" target="_blank" rel="noopener">Pay Less Attention with Lightweight and Dynamic Convolutions</a></em></td>
</tr>
</tbody>
</table>
<h4 id="dynamic-convolutions">Dynamic Convolutions</h4>
<p>Dynamic convolutions are an extension of lightweight convolutions, where at each time step a different convolution kernel is created using a linear function. The weights depend only on the current location, not using global context. As indicated below, the dynamic convolution is crucial to get state-of-the-art performance without using self-attention.</p>
<h3 id="results">Results</h3>
<p>They compared the lightweight and dynamic convolutions to the Transformer Big model. For the convolution models, they swapped out the self attention layers with convolution, and increased the blocks increased to 7 in order to keep the number of parameters in both models consistent. Kernel sizes are increased in the deeper blocks, with kernel sizes of  of 3, 7, 15 and 31x4, and $$H=16$$ for all blocks.</p>
<p>Lightweight convolutions are competitive with the best methods despite their simplicity,  and dynamic Convolutions get state of the art on English-German translation. This shows that self-attention isn&rsquo;t necessary, and can be replaced with simpler and less computationally expensive convolutions</p>
<p>In addition, this paper did a great ablation study to see exactly where the benefits of this model come from. They showed:</p>
<ul>
<li>Wide convolution kernels are necessary to replace self attention</li>
<li>Weight sharing ($$H=1024$$ to $$H=16$$) doesnâ€™t hurt performance</li>
<li>Self-attention works even with limited context size</li>
<li>Dynamic convolution helps, but softmax normalization is needed for convergence</li>
</ul>
<table>
<thead>
<tr>
<th align="center"><img src="../ablation.png" alt="ablation.png"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><em>Proper scientific method</em></td>
</tr>
</tbody>
</table>
<h3 id="thoughts">Thoughts</h3>
<ul>
<li>
<p>It seems an important result here is wider context is important at higher layers, but not important at lower ones. Their dynamic convolution is somewhat similar to self attention, and both still perform well with the limited context size, so long as the context progressively increases.</p>
</li>
<li>
<p>Current SOTA for adversarial images on imagenet uses self-attention layers. Is is possible that these could be replaced using wider, or possibly dynamic convolutions at higher layers?</p>
</li>
</ul>

    </div>

    







<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/&amp;text=Attention%20is%20Overrated" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/&amp;t=Attention%20is%20Overrated" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Attention%20is%20Overrated&amp;body=https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/&amp;title=Attention%20is%20Overrated" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Attention%20is%20Overrated%20https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/&amp;title=Attention%20is%20Overrated" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu2b8d075eea2632f115321e025ae11a3e_74778_270x270_fill_q90_lanczos_center.jpg" alt="Rene Bidart">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://renebidart.com">Rene Bidart</a></h5>
      <h6 class="card-subtitle">PhD Candidate</h6>
      <p class="card-text">PhD candidate at University of Waterloo studying Machine Learning</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:renebidart@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=8TIUlYkAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/renebidart" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>












  
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
