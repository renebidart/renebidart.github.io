<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Academic</title>
    <link>https://renebidart.com/post/</link>
      <atom:link href="https://renebidart.com/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 22 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>https://renebidart.com/post/</link>
    </image>
    
    <item>
      <title>Open Research 8: Optimizing Affine Transforms in VAEs</title>
      <link>https://renebidart.com/post/2020-03-22-03-or8/2020-03-17-open-research-7/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2020-03-22-03-or8/2020-03-17-open-research-7/</guid>
      <description>&lt;h3 id=&#34;optimizing-affine-transforms-in-vaes&#34;&gt;Optimizing Affine Transforms in VAEs&lt;/h3&gt;
&lt;p&gt;Our goal is to create a VAE which will only encode a subset of affine transforms, but generalize to all. This is shown below:
&lt;img src=&#34;../avae_3d.png&#34; alt=&#34;avae_3d.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://renebidart.com/post/or5/2020-03-02-open-research-5/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Previously&lt;/a&gt; we showed takes a higher capacity VAE to encode all possible orientations of an object, compared to encoding a single orientation. Our goal is to create a smaller model by only encoding a subset of all orientations, but still generalize to all orientations at inference. But how can we do this, given a dataset at random orientations?&lt;/p&gt;
&lt;p&gt;Because the VAE is a generative model, we can the optimize an affine transform to find the orinetation at which the VAE best encodes the data, or has lowest loss. Our approach is to use an inner optimization loop during training to optimize this affine transform. The goal is that this will force the model to encode only a subset of the distribution. Then at inference time, this affine optimization can be used to allow the model to generalize to all orientations.&lt;/p&gt;
&lt;p&gt;Normally stochastic gradient descent is used to update the parameters of the network to reduce the loss of the VAE. Here we will take another approach, where the forward pass of the VAE will include an optimization over a set of parameters, where the parameters are optimized to reduce the VAE&amp;rsquo;s loss, maximizing the ELBO. These are the parameters of an affine transform, so the process corresponds to finding the optimal orientation to encode an object at. This optimization objective is shown as:&lt;/p&gt;
&lt;p&gt;$\underset{\alpha}{\text{argmin}}\{ L_{VAE}[\tau_{\alpha}^{-1}(p_\rho(q_\phi(\tau_{\alpha}(x))))]\}$&lt;/p&gt;
&lt;p&gt;To train a network of this form is very expensive, because the above optimization must be solved once for each forward pass. As I discussed 
&lt;a href=&#34;https://renebidart.com/post/or7/2020-03-17-open-research-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;before&lt;/a&gt;, optimizing $\alpha$ is differentiable, but is likely to get caught in a local optima. To overcome this, we evaluate the performance at a number of different random initializations of $\alpha$, and then select the top k of these initializations to do gradient descent on. Here we used 32 random initializations, and did gradient descent on the top 8 of these.&lt;/p&gt;
&lt;p&gt;These tests take a while to run, so we&amp;rsquo;ll look at results later, and give a rigorous interpretation of the above in terms of variational inference.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Research 7: Differentiable Affine Transforms</title>
      <link>https://renebidart.com/post/or7/2020-03-17-open-research-7/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/or7/2020-03-17-open-research-7/</guid>
      <description>&lt;h2 id=&#34;differentiable-3d-affine-transforms&#34;&gt;Differentiable 3D Affine Transforms&lt;/h2&gt;
&lt;p&gt;Affine transforms were made popular for deep learning in 
&lt;a href=&#34;https://arxiv.org/abs/1506.02025&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatial Transformer Networks&lt;/a&gt;, where they introduced an affine transform layer with to transform an image to some cannonical orientation before it was processed by the network. The parameters of the affine transform would be generated by another network, a localization net, that would take the original image as input, and output the 6 parameters of the affine transform.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../spatial_transformer.png&#34; alt=&#34;spatial_transformer.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The interesting part was that the transform was made differentable using bilinear sampling, so it could be trained along with the rest of the network using gradient descent.&lt;/p&gt;
&lt;p&gt;For this work I&amp;rsquo;ll be using variational autoencoders instead of standard CNNs, and 3D instead of 2D transforms. Before we start with this, we should check if these affine transforms can actually be optimized with gradient descent.&lt;/p&gt;
&lt;h3 id=&#34;optimizing-3d-affine-transforms-with-gradient-descent&#34;&gt;Optimizing 3D Affine Transforms with Gradient Descent&lt;/h3&gt;
&lt;p&gt;Gradient descent has no guarantees of finding a global minimum, so we should do some empirical tests to see when this works, or when a bad local minima is reached. Given an object, $x$, and an affine transformed version of this object, $x_\alpha$, we would like to minimize the mean squared error between $x$ and $x_\alpha$, by applying an affine transform to $x$.&lt;/p&gt;
&lt;p&gt;The solution is straightforward, given an affine transform $\alpha$, we could take the matrix inverse $\alpha^{-1}$. In this case we are testing how well the differentiable optimization works, so will optimize an affine transform $\tau_\alpha$, and evaluate how close the local minima reached is to the global optima:
$$\arg\min_\alpha |(x_\alpha-\tau_{\alpha}(x))|$$&lt;/p&gt;
&lt;p&gt;We tested this for rotations, translations, and rotations + translations.&lt;/p&gt;
&lt;p&gt;We found for rotations, the global optima can be reached for roations around $15^\circ$ for simple objects like a dresser (basically a cube), but this is less consistent with more complex objects like a chair or desk. Rotations of over $45^\circ$ rarely find the global optimia for any object:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Object&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;$15^\circ$ mse&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;$30^\circ$ mse&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;$45^\circ$ mse&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sofa&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3.2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;13.2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;30.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dresser&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;15.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Night stand&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;64.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Translations seem to be easier to optimize, and translations of $20%$ of the with can be optimized nearly perfectly. Rotations + translations was extremely difficult, and any substantial rotation + translation meant that frequently the global optima wasn&amp;rsquo;t reached, even for simple objects.&lt;/p&gt;
&lt;p&gt;For this reason we will have to incorporate random restarts into the optimization process, because gradient descent alone frequently is caught in local minima when optimizing affine transforms.&lt;/p&gt;
&lt;p&gt;Below are some examples of a chair at different rotations, and the orientation the optimization ended at:
&lt;img src=&#34;../rotation-opt-chair.png&#34; alt=&#34;rotation-opt-chair.png&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Research 6: Why Deep Learning Research Doesn&#39;t Matter</title>
      <link>https://renebidart.com/post/or6/2020-03-09-open-research-6/</link>
      <pubDate>Mon, 09 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/or6/2020-03-09-open-research-6/</guid>
      <description>&lt;h2 id=&#34;why-deep-learning-research-doesnt-matter&#34;&gt;Why Deep Learning Research Doesn&amp;rsquo;t Matter&lt;/h2&gt;
&lt;p&gt;I started a PhD in deep learning because it was a fast moving field that was having growing impact on the world, and dreams of even greater developments. Even without any AGI dreams, deep learning would transform many industries. So by doing deep learning research I would maximize my impact, right?&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve slowly realized how incorrect this assumption was. While the field as a whole is incredibly important, the impact of most research is negligible. The average researcher produces almost nothing of value, and academia ends up being an intense fight over credit assignment.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use our imagination and pretend I&amp;rsquo;m a top researcher for a minute. What kind of impact can I have if my wildest delusions come true?&lt;/p&gt;
&lt;h3 id=&#34;overestimation-of-impact&#34;&gt;Overestimation of Impact&lt;/h3&gt;
&lt;p&gt;Even the most significant work in our field, like 
&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlexNet&lt;/a&gt; or 
&lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resnets&lt;/a&gt; were not particularly novel. 
&lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resnets&lt;/a&gt; are really a cleaned up version of 
&lt;a href=&#34;https://arxiv.org/abs/1505.00387&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Highway Networks&lt;/a&gt;. As impressive as 
&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlexNet&lt;/a&gt; was, 
&lt;a href=&#34;https://arxiv.org/abs/1202.2745&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dan Cireşan&amp;rsquo;s GPU accelerated CNNs&lt;/a&gt; also won multiple image recognition competitions around the same time. Even GANs were simultaneously discovered by Ian Goodfellow and 
&lt;a href=&#34;https://arxiv.org/abs/1407.4981&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael U. Gutmann&lt;/a&gt;. The reality is 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Multiple_discovery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;multiple discoveries&lt;/a&gt; of the same concept are so frequent it seems any individual is far from irreplaceable.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s easy to forget that &lt;strong&gt;credit assignment is often a winner-takes-all game&lt;/strong&gt;, so we don&amp;rsquo;t see many other less famous people&amp;rsquo;s work, and assume the winner was the only person enlightened enough to tackle the problem. The reality is even top researchers only move the field forward a small amount, which is not a good sign for the impact of an average researcher.&lt;/p&gt;
&lt;p&gt;The lesson for people doing research is that &lt;strong&gt;communication skills are are just as important as research skills&lt;/strong&gt;, and credit assignment should be a key focus of everyone doing research, not just those in reinforcement learning.&lt;/p&gt;
&lt;p&gt;But machine learning has been massively transformative, how can it be that nobody&amp;rsquo;s research is really that indispensable?&lt;/p&gt;
&lt;h3 id=&#34;marginal-vs-total-impact&#34;&gt;Marginal vs. Total Impact&lt;/h3&gt;
&lt;p&gt;The marginal impact of another average machine learning researcher on the world is insignificant.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not arguing that the entire field of machine learning research does nothing important. In my biased opinion deep learning research has had a massive effect so far, and even if research stopped today, this will continue as we get better at reformulating the shiniest new methods in reinforcement learning for population control and finding hacks to better integrate 98% accuracy approaches with humans in problems like medical imaging.&lt;/p&gt;
&lt;p&gt;And this brings us to one of the few useful lessons from economics, the difference between marginal and total benefits:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../marginal_total.png&#34; alt=&#34;marginal_total.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As more and more researchers enter the field, they individually do less and less, even though the field as a whole achieves more. People keep unknowingly redoing each other&amp;rsquo;s work, and still compete to differentiate this work, so there is a massive amount of duplicated work and noise as the number of researchers increase.&lt;/p&gt;
&lt;p&gt;Another way to look at this is that if there were many fewer people working in deep learning, increased basic research would have much higher impact. In terms of business, 
&lt;a href=&#34;https://www.youtube.com/watch?v=3Fx5Q8xGU8k&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;competition is for losers&lt;/strong&gt;&lt;/a&gt;, and that&amp;rsquo;s exactly what machine learning research is today. It is a narrow competition, and unless you do sufficiently weird work, or really are the 1%, impact will be limited.&lt;/p&gt;
&lt;h3 id=&#34;how-to-create-value&#34;&gt;How to Create Value?&lt;/h3&gt;
&lt;p&gt;Impact isn&amp;rsquo;t the only thing we care about, so there&amp;rsquo;s nothing morally wrong with working on a problem because you find it cool. Also, if you do make it to the top 1%, because it is competitive and popular area there&amp;rsquo;s also a lot of social capital / income to be gained. Otherwise, it seems the best way to increase rewards is to do weirder research, so there is less competition.&lt;/p&gt;
&lt;p&gt;The problem with blindly setting $\epsilon$ higher is more frequent failure. And failure&amp;rsquo;s going to look much worse, instead of producing incremental research, you&amp;rsquo;re an outsider who doesn&amp;rsquo;t even belong in the field. It&amp;rsquo;s hard to avoid 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Streetlight_effect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;looking under the streetlight&lt;/a&gt; when everywhere else is darkness. And here we have to build the streetlight before looking under it.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s no hack, more exploration means more risk. But without a significant chance of failure, is it really research? This is good for the field as a whole, or as a professor directing the work of 10 students over a long period of time, but for a young researcher starting off this strategy has massive variance in returns.&lt;/p&gt;
&lt;p&gt;My speculation is the only option is to be the absolute top 1% at research, or to become an expert in something else (probably software development + domain knowledge). The marginal impact of more domain knowledge for a problem is far more important than more deep learning knowledge for the average PhD student. As deep learning becomes more commoditized, as more and more is hidden under APIs, these specialized skills will have less value. Top deep learning researchers will continue to make a ton of money. Improving process for companies at massive scale creates massive value, so companies will compete for the best people.&lt;/p&gt;
&lt;p&gt;Not to mention that creating value is not the same as capturing it, and research is the prime example of this. Even the best researchers don&amp;rsquo;t see significant value from their work. At most, good research is useful for signaling, so they can be indirectly rewarded through a better paying job or tenure.&lt;/p&gt;
&lt;h3 id=&#34;what-will-i-do&#34;&gt;What will I do?&lt;/h3&gt;
&lt;p&gt;My perspective on research has changed. Now it&amp;rsquo;s just another job, where pay has been traded to work on something interesting. I&amp;rsquo;ll still going to work on deep learning, I find it interesting and I want to finish my current projects. I&amp;rsquo;ll continue setting $\epsilon$ a little higher by making research weirder and side projects.&lt;/p&gt;
&lt;p&gt;I didn&amp;rsquo;t realize this thinking about my own work. First had a slightly weaker version of this red pill realizing the people who control the direction of the army of PhD students / researchers are the ones who matter. Through having the power to directly tell us what to work on, like a supervisor or CEO, or even at a more abstract level by changing what is fashionable or good/evil. These ideas can have a massive impact, and we&amp;rsquo;re often unaware its happening. There is no shortage of smart people willing to work very hard who think they are impervious to being infected with an ideology.&lt;/p&gt;
&lt;h2 id=&#34;slacking&#34;&gt;Slacking&lt;/h2&gt;
&lt;p&gt;I didn&amp;rsquo;t get so much done this week, hence the rant rather than some insight about disentangled representations. Really hope this isn&amp;rsquo;t just myself rationalizing my mediocre performance as a PhD student by saying research doesn&amp;rsquo;t matter.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Research 5: 3D Disentangled Representations and Iteration Speed</title>
      <link>https://renebidart.com/post/or5/2020-03-02-open-research-5/</link>
      <pubDate>Mon, 02 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/or5/2020-03-02-open-research-5/</guid>
      <description>&lt;h2 id=&#34;3d-disentangled-representations&#34;&gt;3D Disentangled Representations&lt;/h2&gt;
&lt;p&gt;Last week I started my work on this by implementing a 3d VAE, and now it&amp;rsquo;s time to do something with it. The goal is to disentangle the orientation of an object from its shape, by optimizing over the orientation during training. But the plan is to start with the smallest problem possible, and quickly prove or disprove a hypothesis. I discuss why I think this approach is so important below.&lt;/p&gt;
&lt;p&gt;A simple version of the problem is to limit the transforms to $90^{\circ}$ rotation. Ignore translations, scaling, or any messier rotation. Because of this small space of only 24 orientations brute force optimization is possible, so more difficult gradient descent optimization can be avoided until later. This is because although optimizing affine transforms is differentiable, local maxima are a serious problem, so optimizing affine transforms with SGD be difficult. This simplified case will still let us test the core questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Is is easier (lower loss) for a VAE to encode a subset of orientations of an objects, rather than the full set?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Is it possible to have a VAE learn this subset of orientations, by using an inner loop optimization process over the orientation before gradient descent is applied?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Learning only a subset of orientations is a form of disentangling the orientation and shape, because we will break the latent space into two parts, one to encode the orientation, and another to encode the object.&lt;/p&gt;
&lt;h4 id=&#34;single-orientation--many-orientations&#34;&gt;Single orientation &amp;gt; Many Orientations?&lt;/h4&gt;
&lt;p&gt;To show this disentangled representation is useful, I trained VAEs on datasets of all 24 orientations, as well as on a single orientation, and evaluated how they generalized to a test set of a similar distribution. Not surprisingly, the VAE trained on the single orientation performed better (lower reconstruction MSE). We could explain this by saying the limited capacity of a model meant it was more difficult to learn a more varied distribution of all orientations, rather than the single orientation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../mse_90rot_10c.png&#34; alt=&#34;mse_90rot_10c.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Above we see how performance is lower across the board for different latent sizes with the full set of orientations compared to the single one.&lt;/p&gt;
&lt;p&gt;If we are given a dataset of a single orientation, we can create a model for this single orientation, given a randomly oriented test image, we can optimize the rotation to find the orientation the model was trained on. In this toy example of using $90^{\circ}$ rotations, we will test each of the 24 rotations, and take the rotation with lowest loss as the one the model was trained on. We can show this is equivalent to optimizing the ELBO, but I&amp;rsquo;ll leave that for later.&lt;/p&gt;
&lt;h4 id=&#34;disentangled-3d-representations&#34;&gt;Disentangled 3D Representations&lt;/h4&gt;
&lt;p&gt;Sadly datasets aren&amp;rsquo;t always packaged up nicely in a single orientation, so we&amp;rsquo;d like a model that can learn from random orientations, and still nicely generalize by only learning a single one, and optimizing over the rotation. We&amp;rsquo;ll try a basic method to do this, without any justification why it might work. Before each backward pass of SGD, we&amp;rsquo;ll optimize the orientation, and hope this encourages the model to only learn a subset of rotations.&lt;/p&gt;
&lt;p&gt;The image below shows some examples of the randomly oriented images fed to the VAE on the left, and on the right the rotation the VAE learned.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../mse_90rot_opt_vsno_10c.png&#34; alt=&#34;mse_90rot_opt_vsno_10c.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;And here is a graph of the MSE, with and without this orientation optimization. The loss looks similar to the loss in the above graph, where the model was trained without any augmentation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../sofa_rotated.png&#34; alt=&#34;sofa_rotated.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;This isn&amp;rsquo;t much of a proof of anything, so next week I&amp;rsquo;ll aim to get a couple metrics on this, as well as trying some more general transforms.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;iteration&#34;&gt;Iteration&lt;/h2&gt;
&lt;p&gt;I have spent too many times either philosophizing about a problem and not actually producing anything, or grinding away at a problem for too long without stopping to re-evaluate if that is really what I should be working on. The key is balancing both these things, by spending time narrowly focused trying to finish, but consistently reflecting on results and what matters.&lt;/p&gt;
&lt;p&gt;I also find it helps to have a more clear distinction between doing something, and deciding what to do (or reflecting on a result to decide what to do next). Iteration helps with this, separating work into hypothesis and experiment.&lt;/p&gt;
&lt;p&gt;Interestingly I also talked to a friend who is working on a startup, and he made this lesson more clear to me. He was improving his startup faster by getting more frequent feedback, rather than only focusing on the product. It&amp;rsquo;s much more effective to get real world feedback about your idea than to spend time theorizing about how it will work.&lt;/p&gt;
&lt;p&gt;I made iteration speed the main goal this term (along with increasing the feedback I get while iterating, which is why I&amp;rsquo;m writing this). I thought it was so important that I literally wrote &amp;ldquo;iterate&amp;rdquo; it on my wall. A little weird, but less weird than the &amp;ldquo;increase variance in outcomes&amp;rdquo; I had on my wall last year.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Research 4: Abandoning Weight Sharing, Bitter Lessons and Reasonable Ambition</title>
      <link>https://renebidart.com/post/or4/2020-2-23-open-research-4/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/or4/2020-2-23-open-research-4/</guid>
      <description>&lt;h2 id=&#34;disentangled-representations-bitter-lessons-and-the-edge-of-ability&#34;&gt;Disentangled Representations, Bitter Lessons, and the Edge of Ability&lt;/h2&gt;
&lt;p&gt;For a long time I&amp;rsquo;ve been obsessed with the idea of learning disentangled representations of the world. The general problem of learning disentangled representations is 
&lt;a href=&#34;https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;impossible without assumptions&lt;/a&gt;, just like how there is 
&lt;a href=&#34;https://www.mitpressjournals.org/doi/abs/10.1162/neco.1996.8.7.1341&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;no classsifier to rule them all&lt;/a&gt;. Luckily just like we can construct useful classifiers by making some general assumptions about the structure of the world, we can make assumptions to learn disentangled representations.&lt;/p&gt;
&lt;p&gt;I previously worked on learning disentangled representations for rotations on 2d images, but I&amp;rsquo;m more interested in the general problem, creating 3d disentangled representations from a 2d image. This is much harder, but luckily there&amp;rsquo;s a nice intermediate problem, which is creating a disentangled representation using a 3d image. I&amp;rsquo;ll focus on this first, it should be a straightforward extension of the previous work.&lt;/p&gt;
&lt;p&gt;I started off creating a 3d VAE, which was surprisingly easy. Not much more that changing nn.Conv2d to nn.Conv2d, and it worked reasonably well. It can encode some objects from the modelnet dataset resobably well, so its time to start a prototype. I&amp;rsquo;ll aim to learn to disentangle the orientation of an object, so the model which is fed images at all orientations should learn to encode a single one, plus the orientation angle.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The 
&lt;a href=&#34;https://github.com/NVIDIAGameWorks/kaolin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaolin library&lt;/a&gt; is awesome for 3D data, but unfortunately it can require changing NVIDIA drivers, which is always surprisingly painful&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;generality-and-the-bitter-lesson&#34;&gt;Generality and the Bitter Lesson&lt;/h4&gt;
&lt;p&gt;The real dream is unsupervised learning with less assumptions, where the unsupervised objective is determined only by performance on some downstream tasks. This would be learned self-supervised learning, but right now this is too difficult. Naively attacking this will have a double loop optimization problem - one to train an unsupervised objective, and another to train the supervised objective on top of it. Instead, I&amp;rsquo;ll make more assumptions and attack a particular problem.&lt;/p&gt;
&lt;p&gt;I always worry about working on a problem that will be killed by a more general approach, like in 
&lt;a href=&#34;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Richard Sutton&amp;rsquo;s bitter lesson&lt;/a&gt;. Learned features beat out hand designed features for image classification once there was enough compute, data and and some algorithmic advances. You don&amp;rsquo;t want to be the one working on better hand designed features in 2012. This is always what I&amp;rsquo;m trying to balance, being general enough that the result will scale, and specific enough that the problem is approachable now. More specific assumptions make it more likely to work, but less likely to scale up and create a paradigm shift in the field.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Choosing the generality of an approach in deep learning is a specific case of a common choice we make, which is how ambitious we should be?&lt;/p&gt;
&lt;p&gt;Working on something too easy or too hard are both signs of weakness. It is obvious with a problem that is too easy, because you have no chance of failing, but this is also true with problems that are too hard because there is no expectation of success. You have no fear of failing when working on time travel, and no chance of failing trying to get SOTA on a useless dataset with a pre-trained model.&lt;/p&gt;
&lt;p&gt;It is easy to convince ourselves we aren&amp;rsquo;t as good as we really are, and work on something too easy, or be delusional and do something way too difficult,  where we won&amp;rsquo;t be held accountable for the eventual failure. We have to be self-aware enough to know how good we are, and confident enough to work just at the edge of our ability.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;abandoning-weight-sharing&#34;&gt;Abandoning Weight Sharing&lt;/h4&gt;
&lt;p&gt;Also, this week I put the final nail in the coffin in a project that should have been finished long ago. I did a few more experiments to show weight sharing within convolution layers was not useful for image classification, unlike 
&lt;a href=&#34;https://arxiv.org/pdf/1901.10430.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP where this was useful&lt;/a&gt;. I investigated both adding this weight sharing in standard CNNs (ResNet18, ResNet50), as well as in depthwise separable convolutions (Xception) to investigate if the result was better using the spatial or channel wise convolutions. I found it is much better in spatial convolutions, but because these have so few parameters, it doesn&amp;rsquo;t shrink the network in a significant way. The only case where this might be useful is in regularizing a neural network that is way larger than should be used for a given task, but if this is the case you&amp;rsquo;re better off using a smaller network directly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Research 3: Mistakes</title>
      <link>https://renebidart.com/post/or3/2020-2-16-open-research-3/</link>
      <pubDate>Sun, 16 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/or3/2020-2-16-open-research-3/</guid>
      <description>&lt;h2 id=&#34;distractions-and-mistakes&#34;&gt;Distractions and Mistakes&lt;/h2&gt;
&lt;p&gt;The emotional roller coaster of research continues. Previously I was working on a paper where the results were mediocre, but I thought there was something useful there. I wanted to finish some kind of paper, even if it would never make it to a top conference.&lt;/p&gt;
&lt;p&gt;After some wasted time because of Pytorch lighting and the differences between training on CIFAR10 vs. Imagenet, a few experiments convinced me to had found something good. I was getting unexpectedly good results, where it looked like I&amp;rsquo;d actually found a better way to prune networks. This quickly came crashing down when I realized a coding error had invalidated every promising result. Basically I missed a downsampling layer when implementing weight sharing, and this guaranteed results would look favorable for my method. All my good results were gone, as well as my pointless theorizing about why this was happening. What went wrong?&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;two-sides-of-research&#34;&gt;Two sides of research&lt;/h3&gt;
&lt;p&gt;There are two sides of research - trying to prove you’ve made something good, and trying prove it wrong. This is the scientific method. My issue was I was that I found it difficult to switch between these two modes, so I would do this too slowly. I had done multiple incorrect tests, and not noticed the mistakes until much later. It was carelessness, I knew I had to do test these things, this, but I put it off doing it for too long.&lt;/p&gt;
&lt;p&gt;In general, &lt;strong&gt;iteration speed is key in research, and fast iteration speed means quickly generating and testing hypothesis&lt;/strong&gt;. Effective testing is uncomfortable, because you must try to disprove your work.  The key here is quickly switching between these ways of looking at it - I always wanted to work on it for weeks before attacking it in any way - but this can end in tons of wasted work. I had this tendency to not properly dig into things that are either difficult or might destroy my plan. An even better way is asking others to pick apart your ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Research 2: Getting Started - Self-supervised Learning Basics, Pytorch Ignite, Lightning and Fastai</title>
      <link>https://renebidart.com/post/or2/2020-2-9-open-research-2/</link>
      <pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/or2/2020-2-9-open-research-2/</guid>
      <description>&lt;h2 id=&#34;nlp&#34;&gt;NLP&lt;/h2&gt;
&lt;p&gt;I want to do self-supervised learning, and I think NLP is better suited to this, even though I&amp;rsquo;ve worked on vision previously. There is something special about NLP where simple self-supervised objectives like next word or masked work prediction work well, compared to computer vision where more complex methods are needed. If something works here, I can extend it to vision, and my guess is that the switch to NLP won&amp;rsquo;t be too difficult.&lt;/p&gt;
&lt;h3 id=&#34;frameworks-fastai-ignite-and-lightning&#34;&gt;Frameworks: Fastai, Ignite and Lightning.&lt;/h3&gt;
&lt;p&gt;Iteration speed has been an issue for me. Too much planning and too slow execution. Slow execution was mainly from simple coding mistakes and being too messy. I only learned the absolute minimum of CS needed to get by, so my coding is at the far end of the research spectrum and my projects are often a dumpster fire after a few months.&lt;/p&gt;
&lt;p&gt;I want to use a library both for iteration speed, and hopefully to learn some good coding practices. I spent time setting up my approach so there would be quick iteration speed, and getting familiar with some NLP basics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fastai&lt;/strong&gt; was the best course I&amp;rsquo;ve taken, and library was great for doing standard experiments quickly. Unfortunately I found it difficult to do non-standard things with, and code becomes a mess as soon as you try to change anything. Something about the way it is written means to change things they didn’t explicitly intend to takes a lot of effort. I loved the good defaults and having less to worry about when running experiments, but it not ideal for weird research. Luckily there were a couple more I&amp;rsquo;d never tried before, ignite and lightning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ignite&lt;/strong&gt; is used by 
&lt;a href=&#34;https://github.com/huggingface/transformers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;huggingface&lt;/a&gt;, which probably has the nicest ML code I&amp;rsquo;ve seen, but &lt;strong&gt;Lightning&lt;/strong&gt; has more GitHub stars and more memes in their medium articles!&lt;/p&gt;
&lt;p&gt;I did the Huggingface 
&lt;a href=&#34;https://github.com/huggingface/naacl_transfer_learning_tutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;transfer learning tutorial&lt;/a&gt; using a different dataset with both ignite to see which one I liked better. It was written using ignite, which should have made it easy, but I didn&amp;rsquo;t like the experience. I didn’t find it intuitive. I couldn&amp;rsquo;t quite figure out why, or if I should take the effort to get used to this, so I moved on to Lightning.&lt;/p&gt;
&lt;p&gt;Lightning was more intuitive, but filled with bugs. Even a basic task like mine ran into multiple problems with logging, and lack of documentation. It was annoying because it seemed perfect on the surface. It’s clearly a work in progress but I like the direction, so I’ll stick with it until it becomes limiting.&lt;/p&gt;
&lt;p&gt;I also realized how good huggingface was. Their transfer learning tutorial was amazing, and their code is so clear and concise compared to most I&amp;rsquo;ve seen. This makes a big difference in getting up to speed with NLP.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;meta-how-to-have-a-bad-career&#34;&gt;Meta: How to have a bad career?&lt;/h3&gt;
&lt;p&gt;I saw David Patterson&amp;rsquo;s talk 
&lt;a href=&#34;https://www.youtube.com/watch?v=Rn1w4MRHIhc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Have a Bad Career&lt;/a&gt;, which feels like a more practical and modern version of 
&lt;a href=&#34;https://www.youtube.com/watch?v=a1zDuOPkMSw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You and Your Research&lt;/a&gt;. I&amp;rsquo;ll make sure to revisit this one.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s tons of practical advice on making presentations, giving talks, etc, but there were two big things that stood out for me:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spend as much time polishing and presenting the research as you did on the original research&lt;/strong&gt; If people don&amp;rsquo;t know about your work, or don&amp;rsquo;t understand it, its the same as not having done it. This is totally contrary to the way I used to work, where I would spend the absolute minimum time possible on presentation. To me this was bullshit, I&amp;rsquo;m not a used car salesman, so my work will speak for itself. But I was wrong, and its time to treat presentation as a central part of my work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frequent feedback is key&lt;/strong&gt; Just like in reinforcement learning, delayed rewards are a huge issue with research. The best way to improve research is an another perspective, from someone who will be critical with your work. Even better to occasionally get feedback from people even further from your topic for a wider perspective.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Research 1: Decisions</title>
      <link>https://renebidart.com/post/or1-decisions/2020-1-26-open-research-decisions/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/or1-decisions/2020-1-26-open-research-decisions/</guid>
      <description>&lt;h2 id=&#34;what-to-work-on&#34;&gt;What to work on?&lt;/h2&gt;
&lt;p&gt;I spent the fall doing an internship at a hedge fund (Shell Street Labs) in Hong Kong, which was a great break from this open ended research. But what will I work on now?&lt;/p&gt;
&lt;p&gt;What is the most important problem in machine learning right now? I&amp;rsquo;m excited for what&amp;rsquo;s going to happen in a bunch of fields, like self-supervised learning, time series, 3D modelling, pose estimation, and a more general form of Neural Architecture Search. But which one is right to work on? Excitement alone isn&amp;rsquo;t enough, it better be something I&amp;rsquo;m able to solve. Here are a few of the topics I&amp;rsquo;m most interested in:&lt;/p&gt;
&lt;h3 id=&#34;time-series&#34;&gt;Time Series&lt;/h3&gt;
&lt;p&gt;This internship introduced me the world of time series data, and the significant limitations we have in the field. Time shouldn&amp;rsquo;t be treated the same as another x or y direction. The structure of a problem changes over time and the model needs to know about this, kind of like the state in a hidden markov model. It isn&amp;rsquo;t the same as a language model, the world really is a different place in 2008 than 2020, and we need a learned way to incorporate this into our models.&lt;/p&gt;
&lt;p&gt;We also don&amp;rsquo;t know how what the correct inductive bias is for these type of models. With infinite data transformers are the best approach, because they will change from the bag of words style model to whatever is the correct way to handle time series during training. On the other hand RNNs start out with a better inductive bias, but lack flexibility. How do we trade this off for different sized datasets? Also, how to handle multiple simultaneous series in a way that extends to hundreds/thousands efficiently? These are more open ended questions, but I&amp;rsquo;m surprised there isn&amp;rsquo;t more research on this topic.&lt;/p&gt;
&lt;h3 id=&#34;disentangled-representations&#34;&gt;Disentangled Representations&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve worked a on creating disentangled representations for 
&lt;a href=&#34;https://arxiv.org/abs/1905.05300&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2D rotations before&lt;/a&gt;, but I wasn&amp;rsquo;t sure how to extend the work to 3D. With the release of better libraries like 
&lt;a href=&#34;https://github.com/NVIDIAGameWorks/kaolin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaolin&lt;/a&gt;, it might be the right time to do this, creating generative models which explicitly represent (lighting, pose, etc.) parameters, even without any labeled training data. We would create a VAE which takes as input the image of an object, but reconstructs the object using the differentiable 3D rendering, with the loss as the projection of this object back to an image. We would optimize pose parameters (rotation, lighting, etc.) for each example during training and pray it is able to learn a disentangled representation like what happened in the 2d case.&lt;/p&gt;
&lt;h3 id=&#34;self-supervised&#34;&gt;Self-Supervised&lt;/h3&gt;
&lt;p&gt;What I&amp;rsquo;ve thought about the most over the past few months is self-supervised learning. Self-supervised learning was a cornerstone of NLP for the last couple years and Yann LeCunn is finally getting his unsupervised learning revolution in 
&lt;a href=&#34;https://arxiv.org/pdf/2001.07685.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;computer vision too&lt;/a&gt;. What I&amp;rsquo;m most interested in is how we do self supervised learning, and how it is connected to our brain rewarding us with dopamine? Is this our own self-supervised learning, and could this be incorporated into a deep learning algorithm? The dream would be learning self-supervised objectives, but how can we do this in an efficient way?&lt;/p&gt;
&lt;p&gt;An objective can be represented as a neural network, but naively training this is a double loop optimization, where optimizing the self-supervised objective takes a full unsupervised training of the self-supervised and supervised problems, before optimizing the self-supervised objective with some gradient free method. Or maybe there is some more efficient approach by taking the 
&lt;a href=&#34;https://eng.uber.com/generative-teaching-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;derivative through the training process&lt;/a&gt; to update the objective?&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;meta-why-writing&#34;&gt;Meta: Why Writing?&lt;/h3&gt;
&lt;p&gt;I watched 
&lt;a href=&#34;https://www.youtube.com/watch?v=a1zDuOPkMSw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You and Your Research&lt;/a&gt; by Richard Hamming last week, and it was the best talk I&amp;rsquo;ve heard on . It reinforced my view that its extremely important to pick the right problem, and that so many people don&amp;rsquo;t succeed because they miss this crucial step, and focus on unimportant but familiar problems:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The average scientist, so far as I can make out, spends almost all his time working on problems which
he believes will not be important and he also doesn&amp;rsquo;t believe that they will lead to important problems.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Without both picking the right problem, and working very hard, you will not succeed. He also had a comment about how the people who discussed ideas with others were most likely to work on the most important problems:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But I can say there is a pretty good correlation between those who work with the doors open and those
who ultimately do important things, although people who work with doors closed often work harder.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The modern equivalent of the open door is spreading your ideas online. In a way it is even more extreme than the open door policy, because you&amp;rsquo;re open to criticism and input from anyone. It forces you to continually iterate on your ideas until they are good. Like being in a panopticon, this is an uncomfortable thing, but it seems like the right thing to do. There are some more modern takes on 
&lt;a href=&#34;https://www.perell.com/blog/why-you-should-write&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;why you should write&lt;/a&gt;, but this open door is reason enough for me.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll aim to keep an open door by writing about research once per week.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Research 0: Past</title>
      <link>https://renebidart.com/post/or-past/2020-1-25-open-research-past/</link>
      <pubDate>Sat, 25 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/or-past/2020-1-25-open-research-past/</guid>
      <description>&lt;h2 id=&#34;my-deep-learning-phd-journey-so-far&#34;&gt;My Deep Learning PhD Journey (so far)&lt;/h2&gt;
&lt;h3 id=&#34;2016-2017-pre-phd&#34;&gt;2016-2017 Pre-PhD&lt;/h3&gt;
&lt;p&gt;After messing around with some MMA fight prediction in my undergrad I became interested in machine learning, and decided the best route was to take a masters in statistics. I went to my Waterloo after not being able to defer my acceptance to UofT, and being scared of paying rent in Toronto. I had never visited Waterloo, or had much of a clue what a masters degree or research might entail, only the dream of doing machine learning research.&lt;/p&gt;
&lt;p&gt;To my horror the focus of the master&amp;rsquo;s degree was not research, but taking classes and learning statistics!  Nonetheless I neglected my classes enough that it may as well have been. My first research question was how does optimization behave with fewer parameters like in pruning? I had found networks will still occasionally get good accuracy when trained with few parameters, but mostly fail. I have no idea why I was interested in this, but quickly abandoned it thinking it was a waste of time, and went to search for a real research project.&lt;/p&gt;
&lt;p&gt;I was lucky enough to find a great adviser for my research paper, Ali Ghodsi. He introduced me to people working at Sunnybrook Hospital, and I couldn&amp;rsquo;t be more excited. Not only had I found a supervisor who would let me do machine learning, but he was helping me work on a topic I couldn&amp;rsquo;t have been more excited about. I had met people with preventable misdiagnosis before, and obviously deep learning would be the silver bullet to fix this.  We worked on  
&lt;a href=&#34;http://www.renebidart.com/files/nuclei.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;histopathology grading&lt;/a&gt;, and managed to get some decent results, even giving a presentation at SPIE medical imaging. Research was a grind, with me wasting time because of weak coding skills, but I was hooked.  I was convinced I was on the right path, and decided I needed to pursue this, and the best way was with a PhD.&lt;/p&gt;
&lt;p&gt;Although my supervisor was supportive, and research was going reasonably well, it felt wrong to be in the statistics department, where proofs mattered more than results. Also, statistics was one of the more strict PhD programs, with more mandatory exams, more course requirements, and more teaching assistant work. I felt like I shouldn&amp;rsquo;t be there, so I walked over to the ugly part of campus and asked if I could become an engineer. They were open to another academic change, and I joined a PhD in engineering not knowing what Matlab or a fourier transform was.&lt;/p&gt;
&lt;h3 id=&#34;2018-winter-summer-starting-phd&#34;&gt;2018 Winter-Summer: Starting PhD&lt;/h3&gt;
&lt;p&gt;My supervisor would be Alexander Wong, who was experienced in machine learning, the algorithmic and hardware side of medical imaging,  and even a medical startup. This was clearly the right choice, no matter what direction medical imaging went, we would either come up with either solid research, or make something useful. Now to find another project!&lt;/p&gt;
&lt;p&gt;I wandered around the field, messing around with a few problems, but for the imaging problems in particular, I kept coming back to the same issue. It seemed the technology was already good enough to solve these problems, it was a question of getting enough data and designing a good interface with doctors to make it practically useful. My goal was deep learning research, so I assumed I wasn’t the guy for this task. Of course I didn&amp;rsquo;t immediately switch directions, I spent a month or two not wanting to admit this to my supervisor.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;With hindsight, there were many ways to make this work, there were plenty of unexplored problems in interpretability, multimodal and data efficient ML, but I didn&amp;rsquo;t feel confident to tackle the issue of interfacing with doctors, so I gave up. I wanted to tackle the most important problem, and here I thought it was interface/system design&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;2018-fall-medical-imaging-to-adversarial-examples&#34;&gt;2018 Fall: Medical Imaging to Adversarial Examples&lt;/h3&gt;
&lt;p&gt;I decided to look for a research topic in “pure” machine learning, to avoid the situation where data and interface design would be the secret sauce like what I found in healthcare. What was the most important problem to work on?&lt;/p&gt;
&lt;p&gt;An obvious one was adversarial examples - imperceptible perturbations that could destroy our best deep learning models. The current best approach, adversarial training,  was also the most simple one. I wasn’t going to tackle this,  smarter people with more GPUs were already grinding away at it. I&amp;rsquo;d need to do something a little weirder to succeed. My failed experiments with denoising autoencoders reinforced this, so instead I’d use something else for inspiration - our visual system.&lt;/p&gt;
&lt;p&gt;Multiple objects can&amp;rsquo;t exist in the same location, so we shouldn’t have all features active at every location. Maybe sparse activations are the key to adversarial defence? Were adversarial examples somehow sneaking in to activate other feature maps that shouldn’t be active at all?&lt;/p&gt;
&lt;p&gt;I tried an explicit constraint on this, only allowing top x percent of activations to be active in a given spatial location. Instead of using a scalar non-linearity, this was vector to vector. It didn’t help the problem of adversarial examples much, but I noticed you could still train networks with this activation, even removing most ReLUs. I had no idea if this could lead anywhere, so abandoned it. Later I ran into people from Numenta at ICML 2019 who had proved that 
&lt;a href=&#34;https://arxiv.org/abs/1903.11257&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sparse representations are useful&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Another idea was that our brain uses concepts, while CNNs only extract discriminative features, with no model of what is being seen. I was a 
&lt;a href=&#34;https://arxiv.org/abs/1805.09190&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cool paper using VAEs in this way&lt;/a&gt; , but there wasn&amp;rsquo;t a clear approach forward without really good generative models. I had an idea the way forward was to adding a 
&lt;a href=&#34;https://github.com/renebidart/hvae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hierarchy into the VAE&lt;/a&gt; but the initial experiments were failing, and again I felt I was wandering in the dark.&lt;/p&gt;
&lt;h3 id=&#34;2019-winter-adversarial-examples---vae--disentangled-representations&#34;&gt;2019 Winter: Adversarial Examples -&amp;gt; VAE / Disentangled Representations&lt;/h3&gt;
&lt;p&gt;I decided to focus less on adversarial defence, and more on making intuitively appealing models, because it seemed like the issues with CNNs were deep enough that we may need an entirely new approach.&lt;/p&gt;
&lt;p&gt;Neural Nets don&amp;rsquo;t understand rotations, so for a CNN to generalize to rotated images, we normally make it learn the same image at multiple rotations. This seems wrong, and there had been previous work on the topic like 
&lt;a href=&#34;https://arxiv.org/abs/1506.02025&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spatial transformer nets&lt;/a&gt;, which make a single guess at the correct rotation of the image, or 
&lt;a href=&#34;http://proceedings.mlr.press/v48/cohenc16.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;group equivariant CNNs&lt;/a&gt; which explicitly generalize correctly to rotations, but in a way that was unlikely to scale to more complex transforms. I liked the idea that the network would learn some abstract representation, and then perform some &amp;ldquo;mental rotation&amp;rdquo; on it,  like what was discussed by Goeff Hinton in 
&lt;a href=&#34;http://www.youtube.com/watch?v=rTawFwUvnLE&amp;amp;t=19m50s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;What is wrong with convolutional neural nets ?&amp;quot;&lt;/a&gt; This isn&amp;rsquo;t exactly the point of his talk, but I liked the idea of a network that would perform this mental rotation, but not using the one-shot method of spatial transformer nets.&lt;/p&gt;
&lt;p&gt;I tried doing this with a 
&lt;a href=&#34;https://arxiv.org/abs/1905.05300&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VAE on MNIST&lt;/a&gt;, where during training there is an inner optimization loop to optimize the rotation the image was encoded at. This meant the model was continually learning the best rotation to encode each image at, and because of limited model size the model learned to encode all digits of the same class in the same orientation, leading to a disentangled representation of rotation and object.&lt;/p&gt;
&lt;p&gt;This worked reasonably well for MNIST, but how can it be extended to more complex images? 2D rotations aren&amp;rsquo;t so useful in the real world.&lt;/p&gt;
&lt;h3 id=&#34;2019-winterspring-comprehensive-exam&#34;&gt;2019 Winter/Spring: Comprehensive Exam&lt;/h3&gt;
&lt;p&gt;It came time for my comprehensive exam, so I needed an important topic I could work on for a couple years, which would become my thesis. The last project on the AVAE wasn&amp;rsquo;t going to work because I had no idea how to extend it, so yet again I went looking for a project to commit to.&lt;/p&gt;
&lt;p&gt;Even though there was hype around using algorithms to design neural networks, with NAS, the biggest performance improvements in terms of efficiency and accuracy have come from hand designed components, like convolution, separable convolution, and self-attention. Current NAS methods have not quite lived up to the hype, and have mostly been limited to rearranging existing components of networks, not creating entirely new layers. These assumptions were necessary because current NAS methods are far too inefficient do lower-level search for better architectures. The eventual goal of NAS is to search for an architecture without making any assumptions about its structure, evolving a fully connected one into something better. The problem is we have no way to approach something this difficult. How can we simplify this?&lt;/p&gt;
&lt;p&gt;A lot of different layers can be seen as adding sparsity or weight sharing on standard convolution layers. More generally, they are sparsity and weight sharing on self-attention layers. So what if we assume convolution, and find the optimal form of weight sharing and sparsity within this? Maybe this would be tractable?&lt;/p&gt;
&lt;p&gt;Because of the need to pass my comprehensive I ended up looking into methods for sparsification and quantization, to come up with a computationally feasible approach to this. Also, for some experiments I directly extended some forms of weight sharing as seen in 
&lt;a href=&#34;https://arxiv.org/abs/1901.10430&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dynamic convolutions&lt;/a&gt; to vision problems by forcing multiple convolution filters to share weights, which is better than removing filters directly, but not particularly impressive.&lt;/p&gt;
&lt;p&gt;Looking back, I started with the goal of a general NAS algorithm, and ended adding hard-coded weight sharing in convolutions. The need to get actual results changed my problem from something exciting to something I have no particular interest in. The topic is still important, but I&amp;rsquo;m not sure at this moment I have the right approach for this more general problem.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;meta-exploration-exploitation-trade-off&#34;&gt;Meta: Exploration-Exploitation Trade Off&lt;/h3&gt;
&lt;p&gt;It is exciting to have the freedom to choose research projects, but there&amp;rsquo;s a lot of pressure. Choosing the wrong topic means months of wasted effort. If you don&amp;rsquo;t do good research you don&amp;rsquo;t get rewarded, unless you&amp;rsquo;re in love with the idea of being a &amp;ldquo;Dr.&amp;quot;, or are doing doing a PhD for immigration purposes.&lt;/p&gt;
&lt;p&gt;The issue is the exploration-exploitation trade-off. Do we give up on something, because it doesn&amp;rsquo;t look like a good research direction, or follow through with it because starting over again is so risky?  We both need to pick the right topic, and work on it for a long time to do anything great.&lt;/p&gt;
&lt;p&gt;It is hard to implement a simple strategy like epsilon-greedy when the rewards are delayed. I don&amp;rsquo;t have a good intuition if a project is going well, even a month into it. My guess is so far execution has been the limiting factor for me. At the same time I&amp;rsquo;m conflicted, I don&amp;rsquo;t want to produce low quality research for the hell of it, but by abandoning projects halfway through I&amp;rsquo;ve guaranteed that&amp;rsquo;s all I&amp;rsquo;d do.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Affine Variational Autoencoders for Efficient Generalization</title>
      <link>https://renebidart.com/post/2019-5-2-avae/2019-5-2-affine-variational-autoencoders-for-efficient-generalization/</link>
      <pubDate>Thu, 02 May 2019 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2019-5-2-avae/2019-5-2-affine-variational-autoencoders-for-efficient-generalization/</guid>
      <description>&lt;h2 id=&#34;limitations-of-variational-autoencoders&#34;&gt;Limitations of Variational Autoencoders&lt;/h2&gt;
&lt;p&gt;Deep learning models are powerful, but don’t always generalize well to out of distribution samples. For example, people have no problem understanding slightly rotated digits:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../mnist_batch_rot.png&#34; alt=&#34;mnist_batch_rot.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Rotated MNIST&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But Variational Autoencoders (VAE) do not generalize to this change in distribution this well. Look at how the encoding performance decreases as the image is rotated:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../vae_loss_rotation_ex.png&#34; alt=&#34;vae_loss_rotation_ex.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Images after being encoded by VAE after various rotations. Performance decreases with increased rotation.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In a little more detail, look at the loss when MNIST digits are encoded at different rotations:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../vae_loss_rotation.png&#34; alt=&#34;vae_loss_rotation.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;VAE&amp;rsquo;s loss on various rotation.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In the training dataset all numbers are oriented at $$0^{\circ}$$, and it seems this model performs worse as the digits stray further from this distribution. The loss reaches a peak and decreases, because some digits look identical when rotated $$180^{\circ}$$.&lt;/p&gt;
&lt;p&gt;Normally to solve this problem, we use &lt;strong&gt;data augmentation&lt;/strong&gt;, which is basically feeding a neural net images at all possible rotations, and brute forcing it to understand the full distribution. There are some other methods to approach this&lt;sup id=&#34;a1&#34;&gt;
&lt;a href=&#34;#f1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, but data augmentation is by far the most commonly used, and other methods have other limitations.&lt;/p&gt;
&lt;h2 id=&#34;how-to-solvethis&#34;&gt;How to solve this?&lt;/h2&gt;
&lt;p&gt;Why is it that for a machine learning model to generalize to a rotated image, the most popular method is to feed it the same image at a bunch of different rotations? Blindly doing data augmentation increases the model complexity required, because we must learn all these representations at different orientations. We would prefer this to learn a representation with only a single rotation parameter, but it is hard for a standard CNN to learn this. Why is it that we can understand rotated images, but its so hard for neural nets to generalize in this way?&lt;/p&gt;
&lt;p&gt;Neural nets don&amp;rsquo;t have a built in way to understand rotated images like people do, so they fail to generalize the way human vision does. But what if the model would learn some abstract representation, and then perform some &amp;ldquo;mental rotation&amp;rdquo;, something like what was discussed by Goeff Hinton in 
&lt;a href=&#34;http://www.youtube.com/watch?v=rTawFwUvnLE&amp;amp;t=19m50s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;What is wrong with convolutional neural nets ?&amp;quot;&lt;/a&gt;&lt;sup id=&#34;a1&#34;&gt;1&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;One popular approach, the 
&lt;a href=&#34;https://arxiv.org/abs/1506.02025&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatial Transformer Network&lt;/a&gt; attempts to solve this problem by applying a learned affine transform to return the image to its original orientation. As long as the network figures out the correct transform, this is a great solution! Unfortunately if the model is wrong initially, there is no way to go back and improve on the transform used:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../mnist_rotate_150_40.png&#34; alt=&#34;mnist_rotate_150_40.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;The STN is not guaranteed to get the optimal transformation&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We would like to transform the image closer and closer to some &amp;ldquo;cannonical&amp;rdquo; orientation, based on how &amp;ldquo;weird&amp;rdquo; the image looks to us. But in classification, there is no good metric to measure how weird an image is. The best we can do is something like the classification difficulty/entropy of softmax output.&lt;/p&gt;
&lt;p&gt;But when we have a VAE, there&amp;rsquo;s a clear metric for how &amp;ldquo;weird&amp;rdquo; an image is, the loss!  This loss is shown as:
\begin{equation}
-L_{VAE} = E_{z\sim q_\phi}[logp_\rho(x|z)] - KL(q_\phi(z|x) || p_\rho(z))]
\end{equation}&lt;/p&gt;
&lt;p&gt;We can add an affine transform layer before and after the vae, applying a rotation to the input image, and the inverse rotation to the output.  Then, for any image, we can find the rotation that will minimize this loss. It turns out that in the VAE this is actually equivalent to maximizing a lower bound on the likelihood of the data (the ELBO), so this procedure is actually moving the given image towards the training distribution, which is exactly what we&amp;rsquo;d like.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../affine_vae_1d.png&#34; alt=&#34;affine_vae_1d.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Affine Variational Autoencoder (AVAE). VAE with affine transforms added before and after the network.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this form the whole network is nice and differentiable. So given an image, we can optimize $$\theta$$ using SDG, the same way we optimize the parameters of the network! In practice, there&amp;rsquo;s some local minima, so some random restarts is required.&lt;/p&gt;
&lt;p&gt;And using this, we easily generalize to rotations, and have traded off using an increased model complexity for using increased compute. In fact, this can easily be extended past rotations, and be used for more general affine transforms.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../affine_L8_rot_VAE_AVAE_sgd20_r30_600t.png&#34; alt=&#34;affine_L8_rot_VAE_AVAE_sgd20_r30_600t.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Performance of VAE compared to AVAE on rotated MNIST&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The AVAE gives a clear improvement in performance. Also, because we are only forcing the model to learn images at one orientation, the AVAE can be a smaller model compared to a VAE forced to learn the full distribution using data augmentation.&lt;/p&gt;
&lt;p&gt;Note that this requires all images to be given to the model in some cannonical orientation at training time to achieve the full benefit of having a small model and generalizing well. This isn&amp;rsquo;t good! Most datasets aren&amp;rsquo;t made like this, and people don&amp;rsquo;t have this restriction when learning. How can we get around this and learn both a minimal representation on a subset of the data distribution during training, but also generalize to the full distribution at test time?&lt;/p&gt;
&lt;p&gt;An option is to optimize the transform parameters  $$\theta$$ during the training process, where $$\theta$$ is optimized  &lt;strong&gt;before&lt;/strong&gt; SGD is done for each batch of images. The intuition is that because it is optimal for the model to only learn to encode at a subset of the rotations / single rotation, as training happens the model will gradually start to learn to encode at some rotations better than others, and eventually the model will only be training on a single rotation. It turns out that this works, and this process creates a better performing model without any forced increase in model capacity:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../affine_L8_rot_AVAE_compare_sgd20_r30_600t.png&#34; alt=&#34;affine_L8_rot_AVAE_compare_sgd20_r30_600t.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Comparing AVAE performance using the dataset at a single cannonical orientation, at random rotations, and at random rotations using the optimization process during training. This training optimization process achieves lowest loss.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It worked! The loss of this model is lower for all rotations. And to make sure it worked, we can look at the distribution of rotations of the images during the training process:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../rot_batch_opt_1.png&#34; alt=&#34;rot_batch_opt_1.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Distribution of rotations for digit &amp;ldquo;1&amp;rdquo; during training. As training progress, the model learns to encode at specific orientations.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that the model started off with random rotations, but gradually learned it was better to encode them all at a similar rotation. Because this was for 1, and is almost the same when rotated $$180^{\circ}$$, there are two peaks $$180^{\circ}$$ apart.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../rot_batch_opt_69.png&#34; alt=&#34;rot_batch_opt_69.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Comparing distribution of rotations for digits &amp;ldquo;6&amp;rdquo; and &amp;ldquo;9&amp;rdquo; during training. As training progress, the model learns to encode at specific orientations.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can also compare 6 and 9. The model learned to encode these at $$180^{\circ}$$ at degree rotations apart, because these digits are almost the same after being rotated this amount, and so this is the most efficient way to represent them.&lt;/p&gt;
&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ve got an VAE that generalizes to rotation using computation instead of increasing model size! Unfortunately it does take a lot of computation. Inference takes about 50x as long as the standard VAE. This is totally unoptimized, and it would be quite quicker to use an STN inside the optimization process, with only a few random restarts.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;b id=&#34;f1&#34;&gt;1&lt;/b&gt; Note: &lt;em&gt;This isn&amp;rsquo;t really what he is talking about here, capsules are a much more general concept. I&amp;rsquo;m just stealing the idea of transforming the image to some &amp;ldquo;useful&amp;rdquo; frame&lt;/em&gt; 
&lt;a href=&#34;#a2&#34;&gt;↩&lt;/a&gt;
Code available at 
&lt;a href=&#34;https://github.com/renebidart/equivariance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/renebidart/avae&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attention is Overrated</title>
      <link>https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2019-04-26-attention/2019-04-26-attention-is-overrated/</guid>
      <description>&lt;h2 id=&#34;attention&#34;&gt;Attention&lt;/h2&gt;
&lt;p&gt;In the past the standard design of NLP models, was based on recurrent neural networks (RNN)  to ensure the model can encode long range dependencies necessary in language modelling. This assumption was called into question with the development of the 
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer&lt;/a&gt;, a model where instead of RNNs, self-attention layers are used. The transformer is composed of an encoder and decoder stack of alternating pointwise fully-connected and self-attention layers:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;../transformer.png&#34; alt=&#34;transformer.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;:&amp;ndash;:&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;The Transformer. If this isn&amp;rsquo;t familiar check out the 
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; or this great 
&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Since then self-attention has been useful in a variety of fields, including computer vision, achieving state of the art in 
&lt;a href=&#34;https://arxiv.org/abs/1812.03411&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;adversarial defence on imagenet&lt;/a&gt;, as well as in the 
&lt;a href=&#34;https://arxiv.org/abs/1805.08318&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Self-Attention GAN&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The transformer model made us question the standard design of NLP models being focused on recurrent neural networks, but even more recently the idea that attention is the key for NLP has been questioned.&lt;/p&gt;
&lt;h2 id=&#34;limitations-of-attention&#34;&gt;Limitations of Attention&lt;/h2&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT paper&lt;/a&gt; showed us that attention isn&amp;rsquo;t all you need to achieve good results in NLP, because to achieve their state of the art results they leveraged bidirectional LSTMs in addition to the self attention used in the transformer. But later work has shown not only do we need more than attention, we may not need attention at all. First we can review some limitations of attention:&lt;/p&gt;
&lt;h3 id=&#34;quadratic-complexity&#34;&gt;Quadratic Complexity&lt;/h3&gt;
&lt;p&gt;Attention has a quadratic complexity in input length, meaning attention doesn&amp;rsquo;t scale well over long distances.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../att-complexity.png&#34; alt=&#34;att-complexity.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;From the 
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer&lt;/a&gt; paper&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is because for each position (there are n), we need to attend to every other position in the input (there are also n), for a total of d dimensions in each location, so in total there are $$n^2d$$ operations needed, where d is the dimension of the input.&lt;/p&gt;
&lt;h3 id=&#34;limitations-of-attention-for-long-range-dependencies&#34;&gt;Limitations of Attention for Long Range Dependencies&lt;/h3&gt;
&lt;p&gt;People had speculated that the reason attention was so useful was because it could more easily model long range dependencies in the input, but a recent 
&lt;a href=&#34;https://arxiv.org/pdf/1808.08946.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; has shown this to be incorrect.&lt;/p&gt;
&lt;p&gt;By looking at the performance of CNN, RNN and the transformer on on subject-verb agreement task as the distance between the subject and verb is increased, they showed that self-attention&amp;rsquo;s performance degrades faster than CNNs or RNNs. To overcome this limitation in performance for long range dependencies, many self attention heads are needed.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../att-long-rang.png&#34; alt=&#34;att-long-rang.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;*Accuracy on subject-verb agreement task. Attention is worse than CNNs and RNNs! From&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1808.08946.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Targeted Evaluation of Neural Machine Translation Architectures&lt;/a&gt;*&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;the-alternative-lightweight-dynamic-convolutions&#34;&gt;The Alternative: Lightweight, Dynamic Convolutions&lt;/h3&gt;
&lt;p&gt;In 
&lt;a href=&#34;https://openreview.net/pdf?id=SkVhlh09tX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pay Less Attention with Lightweight and Dynamic Convolutions&lt;/a&gt;, they propose an alternative to self in the form of a two modified convolution layers:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Lightweight Convolutions&lt;/strong&gt; - Depthwise separable convolutions with weight sharing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dynamic Convolutions&lt;/strong&gt; - An extension of lightweight convolutions with dynamic weights&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;lightweight-convolutions&#34;&gt;Lightweight Convolutions&lt;/h4&gt;
&lt;p&gt;Lightweight convolutions leverage an innovation that is commonly used in vision to create more efficient architectures, known as Depthwise Convolutions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Depthwise Convolutions&lt;/strong&gt; take one channel as input for each convolution,  in contrast to the standard convolution, which takes all channels as input . Only taking a subset of channels as input massively reduces the number of parameters, and as was seen in the vision literature still provides good performance.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../depthwise.png&#34; alt=&#34;depthwise.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;As shown in 
&lt;a href=&#34;https://arxiv.org/pdf/1704.04861.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MobileNet&lt;/a&gt;, depthwise convolutions apply a single filter for each channel.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Weight Sharing&lt;/strong&gt; across channels is also added to the depthwise convolution to further reduce the number of parameters. They use the same weights across multiple channels, for a total of H independent weights. In the paper they used a value of $$H=16$$, so instead of learning 1024 different filters, only 16 are learned, in the case that there are 1024 channels.&lt;/p&gt;
&lt;p&gt;Using the Depthwise convolution reduces the parameters required from $$d^2k$$ to $$dk$$, where d is the number of channels, and k is filter width. The addition of weight sharing further reduces the number from $$dk$$ to $$Hk$$.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../compare-conv.png&#34; alt=&#34;compare-conv.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Comparison of self attention, Lightweight convolution and dynamic convolution from 
&lt;a href=&#34;https://openreview.net/pdf?id=SkVhlh09tX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pay Less Attention with Lightweight and Dynamic Convolutions&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;dynamic-convolutions&#34;&gt;Dynamic Convolutions&lt;/h4&gt;
&lt;p&gt;Dynamic convolutions are an extension of lightweight convolutions, where at each time step a different convolution kernel is created using a linear function. The weights depend only on the current location, not using global context. As indicated below, the dynamic convolution is crucial to get state-of-the-art performance without using self-attention.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;They compared the lightweight and dynamic convolutions to the Transformer Big model. For the convolution models, they swapped out the self attention layers with convolution, and increased the blocks increased to 7 in order to keep the number of parameters in both models consistent. Kernel sizes are increased in the deeper blocks, with kernel sizes of  of 3, 7, 15 and 31x4, and $$H=16$$ for all blocks.&lt;/p&gt;
&lt;p&gt;Lightweight convolutions are competitive with the best methods despite their simplicity,  and dynamic Convolutions get state of the art on English-German translation. This shows that self-attention isn&amp;rsquo;t necessary, and can be replaced with simpler and less computationally expensive convolutions&lt;/p&gt;
&lt;p&gt;In addition, this paper did a great ablation study to see exactly where the benefits of this model come from. They showed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wide convolution kernels are necessary to replace self attention&lt;/li&gt;
&lt;li&gt;Weight sharing ($$H=1024$$ to $$H=16$$) doesn’t hurt performance&lt;/li&gt;
&lt;li&gt;Self-attention works even with limited context size&lt;/li&gt;
&lt;li&gt;Dynamic convolution helps, but softmax normalization is needed for convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../ablation.png&#34; alt=&#34;ablation.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Proper scientific method&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;thoughts&#34;&gt;Thoughts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It seems an important result here is wider context is important at higher layers, but not important at lower ones. Their dynamic convolution is somewhat similar to self attention, and both still perform well with the limited context size, so long as the context progressively increases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Current SOTA for adversarial images on imagenet uses self-attention layers. Is is possible that these could be replaced using wider, or possibly dynamic convolutions at higher layers?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>3D Pose Estimation Datasets</title>
      <link>https://renebidart.com/post/2018-12-04-pose-datasets/2018-12-04-3d-pose-estimation-datasets/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2018-12-04-pose-datasets/2018-12-04-3d-pose-estimation-datasets/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;2D pose estimation has improved immensely over the past few years, partly because of wealth of data stemming from the ease of annotating any RGB video. 3D pose annotation is much more difficult because accurate 3D pose annotation requires using motion capture in indoor artificial settings.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../pose/mocap.jpg&#34; alt=&#34;mocap.jpg&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Mocap requires artificial settings and unnatural clothing, as shown in the 
&lt;a href=&#34;https://ps.is.tuebingen.mpg.de/pages/motion-capture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Max Plank&lt;/a&gt; lab&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ground truth vicon datasets can be augmented with synthesized poses using different body shapes, background, etc. but still are limited by the original collected data. Alternatively, synthetic datasets can be used to increase variety. In addition, some use RGB cameras and auxiliary measurement to estimate 3D poses in the wild.&lt;/p&gt;
&lt;p&gt;Some promising datasets are 
&lt;a href=&#34;http://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3D Poses in the Wild&lt;/a&gt; for in the wild poses, 
&lt;a href=&#34;https://arxiv.org/pdf/1803.08319.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JTA&lt;/a&gt; if for simulated data, and 
&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6682899&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human3.6m&lt;/a&gt; and 
&lt;a href=&#34;https://cvssp.org/projects/totalcapture/TotalCapture/TrumbleBMVC2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Total Capture&lt;/a&gt; for controlled environments with ground truth labels.&lt;/p&gt;
&lt;h2 id=&#34;ground-truth-datasets&#34;&gt;Ground Truth Datasets&lt;/h2&gt;
&lt;p&gt;These datasets include ground truth, including full 3d skeleton using mocap, which has negligible measurement error. The limitation is mocap tends to only work in controlled environments, so models trained on these datasets may struggle to generalize.&lt;/p&gt;
&lt;p&gt;Alternatively, synthetic data can be generated either based on human data, as in SURREAL, or entirely synthetically using a game engine as in JTA. Both allow for good quality of annotations, but are limited because of the computer rendered graphics rather than true natural images.&lt;/p&gt;
&lt;h3 id=&#34;human36m-large-scale-datasets-and-predictive-methods-for-3d-human-sensing-in-natural-environments-2014&#34;&gt;Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments (2014)&lt;/h3&gt;
&lt;p&gt;This is the standard in 3d pose estimation. A dataset of 11 people doing 17 common poses in an indoor environment, resulting in a total of 3.6 million frames. The following measurements are included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RGB views: 4 standard ones, one with depth&lt;/li&gt;
&lt;li&gt;Exact 3D joint positions from mocap&lt;/li&gt;
&lt;li&gt;Pixel-level body part labels for a subset of the data&lt;/li&gt;
&lt;li&gt;3D laser scans of the actors (done once, not throughout the videos)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This dataset mostly includes everyday poses, not anything sports specific. natural scenarios, but no specific sports, etc.&lt;/p&gt;
&lt;p&gt;They include some mixed reality augmentation for the dataset as well. Details can be found in 
&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6682899&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../36m.png&#34; alt=&#34;36m.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Example poses from Human3.6m&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;humaneva-2009&#34;&gt;HumanEva (2009)&lt;/h3&gt;
&lt;p&gt;Similar to the 3.6m dataset, but more limited. It is actually composed of two datasets, HumanEva I &amp;amp; HumanEva II, both in controlled indoor environments.&lt;/p&gt;
&lt;p&gt;The HumanEva I dataset consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RGB views: 3 standard ones&lt;/li&gt;
&lt;li&gt;60 Hz 3D joint positions from mocap&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HumanEva II is similar, but with the addition of another camera, as well as multi action scenarios including running around a loop performing actions.&lt;/p&gt;
&lt;p&gt;These datasets have more natural clothes, but this comes with the cost of moving sensors, and less accurate mocap data. Details can be found in 
&lt;a href=&#34;http://humaneva.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HumanEva&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../humaneva.png&#34; alt=&#34;humaneva.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Examples from HumanEva II&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;total-capture-3d-human-pose-estimation-fusing-video-and-inertial-sensors&#34;&gt;Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors&lt;/h3&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://cvssp.org/projects/totalcapture/TotalCapture/TrumbleBMVC2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Total Capture&lt;/a&gt; dataset contains indoor multi-view video, IMU, and vicon mocap with ∼1.9M frames. Very constrained environment.&lt;/p&gt;
&lt;p&gt;This dataset is useful to compare 3d pose estimating using IMU and multiview cameras to ground truth using mocap, but again is quite limited by being in a controlled environment.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../total.png&#34; alt=&#34;total.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Different data types contained in Total Capture&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;learning-from-synthetic-humans-2017&#34;&gt;Learning from Synthetic Humans (2017)&lt;/h3&gt;
&lt;p&gt;SURREAL (Synthetic hUmans foR REAL tasks) dataset contains synthetically generated data rendered from 3D sequences of motion capture data. The original data is from Human3.6M.&lt;/p&gt;
&lt;p&gt;The idea is that you can randomly sample the person&amp;rsquo;s pose, appearance, lighting, camera position and background, creating a massive dataset from only a few annotated models. The SMPL model is used to decompose the body into pose and shape parameters, so we can sample these independently to produce an image. To generate realistic shapes, they use the CAESAR dataset to train SMPL, and then randomly select a participant and perturb the shape components slightly.&lt;/p&gt;
&lt;p&gt;Once the person is generated from the random shape and pose parameters, sample camera, texture, light and background to get the full scene. Details can be found in 
&lt;a href=&#34;https://arxiv.org/abs/1701.01370&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning from Synthetic Humans&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../surreal.png&#34; alt=&#34;surreal.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Surreal pipeline for generating data&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;jta-dataset-2018&#34;&gt;JTA Dataset (2018)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../gta5.png&#34; alt=&#34;gta5.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Surreal pipeline for generating data&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Massive simulated dataset created from Grand Theft Auto V. Consists of about 500 000 frames with almost 10 million body poses labeled with full 3D annotation.&lt;/p&gt;
&lt;p&gt;Details can be found in 
&lt;a href=&#34;https://arxiv.org/pdf/1803.08319.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;in-the-wild&#34;&gt;In the wild&lt;/h2&gt;
&lt;p&gt;Instead of using ground truth mocap annotations, annotations are estimated in order to get a wider variety of actions in the wild instead of being constrained to indoor setting with mocap. Ground truth can be estimated using auxiliary measurements such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inertial measurement units&lt;/li&gt;
&lt;li&gt;Multiview cameras&lt;/li&gt;
&lt;li&gt;RGBD cameras&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3d-poses-in-the-wild-dataset&#34;&gt;3D Poses in the Wild Dataset&lt;/h3&gt;
&lt;p&gt;60 in the wild videos with 2D and 3D pose annotation, including camera pose and scanned models with different clothing variations. The poses are estimated using IMUs and a handheld 2d video. This is the only promising 3d pose in the wild dataset.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../wild.png&#34; alt=&#34;wild.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Actual in the wild images with 3d poses&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;On TotalCapture dataset this gets 26mm error, and outperforms their method without using multiview. This is a best case scenario for accuracy because it was in a controlled environment, so we should expect this dataset to have error &amp;gt;26mm. Nevertheless, this could be very useful for situations where absolute accuracy isn&amp;rsquo;t crucial, and we want to generalize to the wild. More details on their 
&lt;a href=&#34;http://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;monocular-3d-human-pose-estimation-in-the-wild-using-improved-cnn-supervision-2016&#34;&gt;Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision (2016)&lt;/h3&gt;
&lt;p&gt;Using multi-camera markerless system, recording 3D pose in green screen to change background.  8 actors, 8 cameras, &amp;gt;1.3M frames. This is still indoor in a relatively controlled environment, so it isn&amp;rsquo;t truly in the wild, and there may be some uncertainty with how accurate their ground truth is. More info in their 
&lt;a href=&#34;https://arxiv.org/pdf/1611.09813.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;unite-the-people-closing-the-loop-between-3d-and-2d-human-representations-2017&#34;&gt;Unite the People: Closing the Loop Between 3D and 2D Human Representations (2017)&lt;/h3&gt;
&lt;p&gt;In 
&lt;a href=&#34;https://arxiv.org/abs/1701.02468&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unite the People&lt;/a&gt; they use RGB to 3D estimation based on CNN, with some manual removal of bad results, and no uncertainty estimate. They tested their model on model on Human3.6m, getting 80mm average error. This is probably best case error, so this isn&amp;rsquo;t useful for most applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Remote Setup</title>
      <link>https://renebidart.com/post/2018-8-14-setup/2018-8-14-deep-learning-remote-machine-setup/</link>
      <pubDate>Tue, 14 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2018-8-14-setup/2018-8-14-deep-learning-remote-machine-setup/</guid>
      <description>&lt;h2 id=&#34;basic-set-up&#34;&gt;Basic Set Up&lt;/h2&gt;
&lt;p&gt;To do any deep learning, you&amp;rsquo;ll either need to have set up a VM at a cloud provider (AWS, Azure or paperspace, google, etc.), unless you happen to have a gpu lying around. Most commonly these will be using a linux distribution like Ubuntu 16.04&lt;/p&gt;
&lt;p&gt;If you aren&amp;rsquo;t familiar with &lt;strong&gt;linux&lt;/strong&gt;, do a basic 
&lt;a href=&#34;https://ryanstutorials.net/linuxtutorial/commandline.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial&lt;/a&gt;. After sshing to the machine, we will set it up:&lt;/p&gt;
&lt;p&gt;There are a few things that must be installed everywhere:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;CUDA&lt;/strong&gt; - GPU software&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tmux&lt;/strong&gt; - Preventing ssh connection from detaching with long running jobs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSH alias&lt;/strong&gt; - connecting quickly&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;1-cuda&#34;&gt;1. CUDA&lt;/h4&gt;
&lt;p&gt;If CUDA isn&amp;rsquo;t already installed, follow the instructions shown in this 
&lt;a href=&#34;https://gist.github.com/wangruohui/df039f0dc434d6486f5d4d098aa52d07&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gist&lt;/a&gt;. This is probably the best way to set this stuff up. Make sure you&amp;rsquo;re installing the latest version, which you can check on the [Nvidia Website] (&lt;a href=&#34;https://www.geforce.com/drivers&#34;&gt;https://www.geforce.com/drivers&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Here is an 
&lt;a href=&#34;http://files.fast.ai/setup/paperspace&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example script&lt;/a&gt; to set up for the 
&lt;a href=&#34;http://www.fast.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast Ai&lt;/a&gt; course. This is an easy way to get everything installed, but may include a bunch of unnecessary things.&lt;/p&gt;
&lt;h4 id=&#34;2-tmux&#34;&gt;2. Tmux&lt;/h4&gt;
&lt;p&gt;Tmux is used to prevent jobs from stopping when your connection to the remote machine is closed, as well as making your screen look cooler. This should be installed using:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install tmux
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;../screen.png&#34; alt=&#34;screen.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;:&amp;ndash;:&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Without Tmux&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../tmux.png&#34; alt=&#34;tmux.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;With Tmux&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Tmux has a strange default configuration, and is difficult to use. You can make tmux much more user friendly, and can even use a mouse mode by using this configuration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/gpakosz/.tmux.git
ln -s -f .tmux/.tmux.conf
cp .tmux/.tmux.conf.local .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each time a you want to run code, start a tmux session using&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tmux
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you log back on to the remote computer, reconnect to tmux using:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tmux a
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More details are 
&lt;a href=&#34;https://github.com/gpakosz/.tmux&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: An alternative is screen. Don&amp;rsquo;t use this, tmux is better.&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;3-ssh-alias&#34;&gt;3. SSH alias&lt;/h4&gt;
&lt;p&gt;There are many cases where a lot of typing can be saved by adding aliases the .bashrc file.
For example, normally you will connect by using ssh:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh username@123.45.678.90
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is hard to remember, and will be annoying to copy paste every time. It would be easier to remember something like ssh-aws, which can be done by adding an alias.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re working on a mac or linux you can make a shortcut instead by adding it to the .bashrc file. If it does not exist create it in your home directory.&lt;/p&gt;
&lt;p&gt;Add this line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alias ssh-aws=&#39;ssh -Y username@123.45.678.90&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;editing-code--running-experiments&#34;&gt;Editing Code / Running Experiments&lt;/h2&gt;
&lt;p&gt;Editing code remotely isn&amp;rsquo;t as straightforward as doing it locally. There are a few commonly used options:&lt;/p&gt;
&lt;h4 id=&#34;1--vim&#34;&gt;1.  VIM&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Edit code directly on the remote computer.&lt;/li&gt;
&lt;li&gt;This is the most commonly recommended approach, but it has a steep learning curve and will take some 
&lt;a href=&#34;https://medium.com/actualize-network/how-to-learn-vim-a-four-week-plan-cd8b376a9b85&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;effort&lt;/a&gt; to become proficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-sshfs-for-mac&#34;&gt;2. SSHFS for mac&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This allows you to mount the remote directly locally, so you can edit the code like any other code on your machine&lt;/li&gt;
&lt;li&gt;Can use any text editor or IDE&lt;/li&gt;
&lt;li&gt;Install following the 
&lt;a href=&#34;https://github.com/libfuse/sshfs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Add an alias to the local .bashrc as:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;alias sshfs-aws=&#39;umount /Users/.../local_sshfs_folder; 
      sshfs username@123.45.678.90:/username/code  /Users/.../local_sshfs_folder&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;3-ides&#34;&gt;3. IDEs&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Some IDEs directly integrate this functionality&lt;/li&gt;
&lt;li&gt;The paid version of 
&lt;a href=&#34;https://www.jetbrains.com/pycharm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pycharm&lt;/a&gt; has a remote sync option.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-scp&#34;&gt;4. SCP&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;It is possible to edit files locally and then scp them to the server, but this is a bad idea. Don&amp;rsquo;t do this.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;organiation&#34;&gt;Organiation&lt;/h3&gt;
&lt;!--| ![file_tree.png](../file_tree.png) |
|:--:| 
| *Example Directory Structure* |--&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../file_tree.png&#34; alt=&#34;file_tree&#34; style=&#34;width:300px;&#34;/&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Example Directory Structure&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;To make it easier to mount the directory remotely, I keep the code for all projects together, and have the datasets in a seperate location.&lt;/li&gt;
&lt;li&gt;Each project has its own git repo.&lt;/li&gt;
&lt;li&gt;Each dataset has its own models stored together, with an explicit test/train/val split&lt;/li&gt;
&lt;li&gt;There is no correct way to do this, and the organization really depends on the complexity of the project. Look at 
&lt;a href=&#34;https://drivendata.github.io/cookiecutter-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cookiecutter datascience&lt;/a&gt; for a good overview of this.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;starting-a-project&#34;&gt;Starting a project&lt;/h2&gt;
&lt;p&gt;Once you have connected to the remote computer and made a tmux session:&lt;/p&gt;
&lt;h4 id=&#34;1-create-a-virtualenvhttpsdocspythonorg3libraryvenvhtml-for-each-project&#34;&gt;1. Create a 
&lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;virtualenv&lt;/a&gt; for each project&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;python3 -m venv ENV
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be activated using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source ENV/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To save the package requirements, use&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip freeze &amp;gt; requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The purpose of a virtualenv is to isolate each of your projects, allowing you to seperately install dependencies for each project without worrying about conflicting requirements. The requiremnts are easily saved using pip freeze, so it is easy for anyone work on the project using a different computer, because all dependencies can be installed with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;2-create-a-git-repohttpsdocspythonorg3libraryvenvhtml-for-each-project&#34;&gt;2. Create a 
&lt;a href=&#34;https://docs.python.org/3/library/venv.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;git repo&lt;/a&gt; for each project&lt;/h4&gt;
&lt;p&gt;Initialize the folder your project is stored in as a git repo using&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git init &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If your aren&amp;rsquo;t familair with git, try this 
&lt;a href=&#34;http://rogerdudler.github.io/git-guide/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial&lt;/a&gt;. Make sure to frequently commit changes and backup to a service like Github, Bitbucket or Gitlab. This will be a lifesaver when code is accidnetally deleted, and will make sharing code and switching computers easy.&lt;/p&gt;
&lt;p&gt;It is useful to include a readme explaing the basics of the code and how to reproduce key results, as well as the requirements.txt file. Working on a project on a new computer will be simple, because all you need to do is clone the repo and install the requirments to get it working.&lt;/p&gt;
&lt;h4 id=&#34;3-jupyter-notebooks&#34;&gt;3. Jupyter Notebooks&lt;/h4&gt;
&lt;p&gt;
&lt;a href=&#34;http://jupyter.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter notebooks&lt;/a&gt; are great for quickly testing ideas and presenting results. They are popular for data science in general, but there can be some issues when running them remotely. There are a few ways to make this work.&lt;/p&gt;
&lt;p&gt;To start a notebook on the remote machine, specify that no browser is being used, as well as the port. Export this in the .bashrc on the &lt;strong&gt;remote&lt;/strong&gt; machine:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;alias notebook=&#39;jupyter notebook --no-browser --port=8889&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once a notebook is started, you must connect the local to it. This is greek to me, but seems to work. Add this to the &lt;strong&gt;local&lt;/strong&gt; .bashrc:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;alias aws-nb=&#39;lsof -ti:8888 | xargs kill -9; 
ssh -N -f -L localhost:8888:localhost:8889 username@123.45.678.90&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note: This may not be the most secure way to do this&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&#34;notebooks-and-virtual-environments&#34;&gt;Notebooks and virtual environments&lt;/h4&gt;
&lt;p&gt;Before starting a notebook, activate the virtual envrionment and run this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install ipykernel&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ipython kernel install --user --name=project1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This ensures the correct kernel will be available from inside the notebook. You should be able to select the kernel &amp;ldquo;project1&amp;rdquo; from inside the notebook.&lt;/p&gt;
&lt;p&gt;The downside of notebooks is they quickly become a mess when there is too much code. It is best to break the code inside the notebook into functions, and keep these in seperate python files outside the notebook. Multiple notebooks can then import the same functions.&lt;/p&gt;
&lt;p&gt;Keep notebooks primarily for exploring data and visualizing results, not for running complex jobs or storing code.&lt;/p&gt;
&lt;h4 id=&#34;notebook-config&#34;&gt;Notebook Config&lt;/h4&gt;
&lt;p&gt;To ensure the notebook plots correctly and reloads updated functions, I add this into a cell at the top of every notebook:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
%load_ext autoreload
%autoreload 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more complex projects, notebooks will quickly become a mess because there is too much code floating around in each notebook. &lt;strong&gt;For functions used multiple times, save them in a seperate .py file&lt;/strong&gt;, and load them into the notebook.&lt;/p&gt;
&lt;p&gt;Notebooks &lt;strong&gt;won&amp;rsquo;t record output on long running processes&lt;/strong&gt; after your computer has disconnected. More complex jobs should  be run using the regualar command line, possibly using 
&lt;a href=&#34;http://sacred.readthedocs.io/en/latest/index.html#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sacred&lt;/a&gt; to ensure reproducability.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Developing on a remote machine with bigger jobs can be difficult, especially if you&amp;rsquo;ve got limited coding background. Reproducability and iteration speed become much more imnportant when dealing with slow running and complex experiments, so its worth it to take the time to figure out a good method to make this happen. What is here was based on my own experiences suffering through a bad workflow and lack of reproducability, and are mostly based off of trial and error.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Good, the Bad and the Weird of Vipassana Meditation</title>
      <link>https://renebidart.com/post/2018-03-26-vipassana/2018-03-26-the-good-the-bad-and-the-weird-of-vipassana-meditation/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2018-03-26-vipassana/2018-03-26-the-good-the-bad-and-the-weird-of-vipassana-meditation/</guid>
      <description>&lt;p&gt;I completed the 10 day silent Vipassana Meditation course at the Dhamma Torana center outside Toronto, Canada. It was my first meditation course, and it was an interesting mix of enlightenment and a big test of self control.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt;&lt;br&gt;
The 10 day Vipassana course is quite unpleasant but totally worthwhile. Yes, you should do it.&lt;/p&gt;
&lt;h2 id=&#34;good&#34;&gt;Good&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Mental Benefits&lt;/strong&gt;&lt;br&gt;
Meditation is peddled as a cure for problems in all aspects of your life. This sounds like bullshit, but because it was recommended by so many respected people I figured it was worth a try.&lt;/p&gt;
&lt;p&gt;They were right. The week following the meditation was far more peaceful than any other in the past few years of my life. I confronted issues that were bothering me, and feel more at peace with myself. I even feel a little more control over my emotions, a little more able to handle awkward or frustrating situations by taking myself out of the moment. It isn&amp;rsquo;t some miracle drug, but it seems to have some significant benefit.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Physical benefits&lt;/strong&gt;&lt;br&gt;
Before the meditation I had terrible posture, muscle imbalance and weakness in my core but but wasn&amp;rsquo;t fully aware of it. This course forces you to confront this, because you&amp;rsquo;ll have plenty of time to focus on you posture while sitting in an uncomfortable position all day. It also seems to do much more than just give you better posture. You develop a better mind muscle connection through the practice, enabling you to better activate muscles, and also be more aware of issues in your body. For me this was huge, and did more for me than seeing actual physiotherapists about my issues. Again, not a miracle drug for this, but it feels like it put me on the right path.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Free&lt;/strong&gt;&lt;br&gt;
The course is totally free, and paid by donation. Sounds too good to be true, but suprisingly there&amp;rsquo;s no fine print and this isn&amp;rsquo;t a scam. You should donate if you find it beneficial, but it will still be the cheapest 10 day &amp;ldquo;vacation&amp;rdquo; of your life.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Not a cult or religion&lt;/strong&gt;&lt;br&gt;
It sounds a little sketchy to go meditate with random people somehere in the woods in Ontario, but as soon as I arrived these fears were gone. The whole center is run by a bunch of volunteers who really just want to spread the benefits of this meditation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Willpower&lt;/strong&gt;&lt;br&gt;
As you&amp;rsquo;ll see below, the 10 days will be a big exercise in strengthing your willpower.&lt;/p&gt;
&lt;h2 id=&#34;bad&#34;&gt;Bad&lt;/h2&gt;
&lt;p&gt;After you leave the meditation center everything looks nicer, colors are brighter and food tastes better. I imagine this is a more mellow version of how a prisioner feels after getting released. You can&amp;rsquo;t help but feel good after being stuck in this environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boring&lt;/strong&gt;&lt;br&gt;
If sitting quietly in a room all day doesn&amp;rsquo;t sound so exciting, you&amp;rsquo;re not going to like this course. Almost nobody I talked to found the course to be enjoyable. This was by far the most boring 10 days of my life. There is almost no mental stimulation, and for the entire time the meditation consists of feeling sensations in some part of your body. To be honest I hadn&amp;rsquo;t even checked what this meditation was before I arrived, so it was a big shock to find I would be noticing sensations around my nose for three full days, and for the next 7 the sensations in my whole body.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Painful&lt;/strong&gt;&lt;br&gt;
Mediation is horrible. No matter how peaceful people may look sitting cross legged, when you discover pains in you back, knees and neck within the first 30 minutes of meditating, you&amp;rsquo;ll have no peace when imagining how the next 10 days will feel.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Free?&lt;/strong&gt;&lt;br&gt;
It may not cost anything, but 10 days is a huge time commitment for anyone with any semblance of a life. This could be over half of your yearly vacation days. It looks a little more costly when you realise you have to give up that vacation somewhere warm to spend 10 days meditating.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Science&lt;/strong&gt;&lt;br&gt;
In the evening videos Goenka (the popularizer of this method) will inform you of how scientific meditation is. Don&amp;rsquo;t get too hopeful, because his explainations won&amp;rsquo;t involve hypothesis, theorems, or empirical evidence. Better to take it as a practical exercise than dig to much into his thoughts about subatomic particles.&lt;/p&gt;
&lt;h2 id=&#34;weird&#34;&gt;Weird&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Chanting&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=wZQBWUkMm1I&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chanting&lt;/a&gt; is pretty strange, and this will happen multiple times per day during the meditations. This part was certainly never altered to make it more palatable for a western audience, so probably it will be strange. I treated it as another exercise to try to remain equanimous during the chanting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where do the benefits come from?&lt;/strong&gt;&lt;br&gt;
I felt better after the meditation, but how much of this was from the mindfulness meditation itself, and how much is from being forced into this unusual environment of no phone, no computer, no talking for 10 days?&lt;/p&gt;
&lt;p&gt;For the time of the course very little outside information is entering your brain, so you have nothing to do but to deal with your own thoughts. This is good for sorting out some issues you may have, and a lot of this will happen even if you don&amp;rsquo;t meditate at all.&lt;/p&gt;
&lt;p&gt;My guess is that the combination of the mindfulness meditation goes very well with this lask of information. You would see benefits from isolating yourself from technology for this time, but when combined with the meditation, you also learn a better way to deal with all the thoughts that are coming toward you.&lt;/p&gt;
&lt;p&gt;There is 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3679190/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;real research&lt;/a&gt; supporting the benefits of mindfulness, and so clearly this is a good idea, I just think it would be interesting to isolate the benefit of removing technology vs. meditation vs. exercising your willpower.&lt;/p&gt;
&lt;h2 id=&#34;should-you-do-it&#34;&gt;Should you do it?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The retreat is like a little reset button for your life.&lt;/strong&gt; It is a great way to generally improve your life, and if you keep up some amount of meditation practice they claim these results will last. It is also easy to start new habits after leaving the meditation, so you are in a very good position to improve yourself in whatever way you see fit.&lt;/p&gt;
&lt;p&gt;In conclusion, I see this meditation as a way of sacrificing 10 days for the chance at living a more peaceful life. This is a good gamble.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sequential Stratified Sampling</title>
      <link>https://renebidart.com/post/2018-01-26-sampling/2018-01-26-sequential-stratified-sampling/</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2018-01-26-sampling/2018-01-26-sequential-stratified-sampling/</guid>
      <description>&lt;h2 id=&#34;stratified-sampling&#34;&gt;Stratified Sampling&lt;/h2&gt;
&lt;p&gt;We want to get the estimate the mean of a population, and this population is broken into different groups. As long as these groups are homogeneous, we can produce a better estimate by estimating the means of the groups individually. For example, we could estimate the mean math ability of students in Ontario by dividing it up by schools. By allocating more samples to the schools with higher number of students and variance in ability, we can get a better estimate overall.&lt;/p&gt;
&lt;p&gt;Back in 1934 
&lt;a href=&#34;http://www.stat.cmu.edu/~brian/905-2008/papers/neyman-1934-jrss.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jerzy Neyman&lt;/a&gt; proved that the optimal way to allocate the number of samples for each stratum, $n_h$ is proportional to its standard deviation, $\sigma_{yh}$, and its size $W_h$:&lt;/p&gt;
&lt;p&gt;$$
n_h \propto W_h\sigma_{yh}
$$&lt;/p&gt;
&lt;p&gt;In reality we won&amp;rsquo;t know the stratum variances, so alternative methods using auxiliary information or two stage sampling to get an estimate of the variances are used. The two sample method is not optimal, and auxillary information may not be availiable.&lt;/p&gt;
&lt;p&gt;Today there are situations where we can take samples sequentially, which may not have been possible when these methods were first developed, such as with an online survey where it is easy to distribute and quick to analyse results, but may be costly to pay an individual to complete it. Here we will allocate each sample individually, but how do we pick which strata to sample from?&lt;/p&gt;
&lt;h2 id=&#34;sequential-sampling&#34;&gt;Sequential Sampling&lt;/h2&gt;
&lt;p&gt;we will think of sampling over a number of timne steps, taking one sample from a given stratum at each time. The goal is to sample from the stratum that will cause the largest expected decrease in variation of our estimate, $\bar{y}$ at each iteration. We would would like to find $i$ to maximize: $$V(\bar{y}&lt;em&gt;{st, i})-V(\bar{y&lt;/em&gt;{st}}) = $$&lt;/p&gt;
&lt;p&gt;$$
\sum\limits_{h=1}^{H}W_{h, i}^2(1-\frac{n_{h,i}}{N_{h,i}})\frac{s_{yh,i}^2}{n_h{,i}}-\sum\limits_{h=1}^{H}W_h^2(1-\frac{n_h}{N_h})\frac{s_{yh}^2}{n_h}
$$&lt;/p&gt;
&lt;p&gt;This says we would like to maximize the decrease in variance&lt;/p&gt;
&lt;p&gt;Here $$\bar{y}&lt;em&gt;{st, i}$$ is used to denote this after another sample is taken from stratum $$i$$. For simplicity, assume that all strata have the same size, so $$W_h$$ are equal. Then, we can notice that all the terms in this sum are equal, except for those relating to stratum $$i$$:
\begin{equation}
(1-(\frac{n_i+1}{N_i}))\frac{s_i^{*2}}{n_i+1} - (1-(\frac{n_i}{N_i}))\frac{s_i^{2}}{n_i}
\end{equation}
If we assume $s_i^{*2}=s_i^{2}$, after rearranging we obtain the objective as:
$\max&lt;/em&gt;{i} \frac{s_i^{2}}{n(n+1)}$&lt;/p&gt;
&lt;p&gt;This shows that the expected benefit is increasing in the variance of the sample, as would be expected from Neyman allocation, and is decreasing in the number of samples already taken from that strata, because as more samples are taken from a given strata, the we have a better estimate of its&amp;rsquo; mean.&lt;/p&gt;
&lt;h2 id=&#34;procedure&#34;&gt;Procedure:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Take a small number of samples from all strata, compute variance, and the expected benefit estimates for all strata.&lt;/li&gt;
&lt;li&gt;Compute the expected benefit for each of the strata&lt;/li&gt;
&lt;li&gt;Sample from the strata with maximum expected benefit&lt;/li&gt;
&lt;li&gt;Repeat this until a total of $n$ samples are taken&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Implementation Detail:  To make this computationally efficient, we can use the iterative update formulas for the mean and standard deviation. Using this means previous samples don&amp;rsquo;t have to be stored.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is a simple way to take advantage sequental sampling using stratified sampling. Compared to the two-sample method, this will more efficiently allocate samples, so should result in lower variance of the estimator. More details are 
&lt;a href=&#34;https://github.com/renebidart/stratified-sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NIPS 2017 Highlights</title>
      <link>https://renebidart.com/post/2017-12-13-nips/2017-12-13-nips-2017-highlights/</link>
      <pubDate>Wed, 13 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2017-12-13-nips/2017-12-13-nips-2017-highlights/</guid>
      <description>&lt;h2 id=&#34;the-hype-continues&#34;&gt;The hype continues&lt;/h2&gt;
&lt;p&gt;Like many of the attendees, this was my first NIPS conference. Overall a great experience, and I doubt there is any substitute for meeting interesting people and learning about the current state of machine learning. Here I’ll mention a few of the highlights of the conference for me, and some of the weirdness.&lt;/p&gt;
&lt;p&gt;Money is flooding in from everywhere in AI, with desperate investors trying to get a piece of the pie. Free alcohol and parties every night was unexpected for an academic conference:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;../flo-rida.png&#34; alt=&#34;flo-rida.png&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;:&amp;ndash;:&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;If this doesn’t signal a bubble I don’t know what does&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With this level of hype there can be some downsides:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;../lineup.jpeg&#34; alt=&#34;lineup.jpeg&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;em&gt;Over 8000 people attended, and they all needed to register&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;rigor-vs-accuracy&#34;&gt;Rigor vs. Accuracy&lt;/h2&gt;
&lt;p&gt;In the last few years the field of machine learning has become obsessed with the performance provided by larger and less interpretable deep neural networks. Not everyone is in love with the hype. According to Ali Rahimi:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Machine learning isn’t the new electricity, it is alchemy.
We should start from simple, easy to understand things and then move onto more complex things. Currently we apply brittle optimization techniques to loss surfaces we don’t understand&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yann LeCun’s response:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We often use methods before we fully understand them in engineering and it has made the world a better place. Don’t throw the baby out with the bathwater&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There was also a debate about model interpretability, where many of the same arguments are thrown around:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Do you really want to be diagnosed by something you don’t understand?
vs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;We will choose an accurate model over an interpretable one when it is our body on the line.
I think when deploying a model for real use we are facing a trade off between a complex model, and an ensemble of a simple model and human judgement. We know the complex model performs better on the test data, but we assume in real use the data distribution will change. The human/simple model will be more robust to changes in data distribution, because of the human’s debugging ability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We must decide if the increased accuracy of the complex model makes up for the danger of it generalizing less well than the human/simple model ensemble. Unfortunately I have no idea how to quantify how much better one model may generalize than the other, so we will continue arguing.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ml-for-healthcare&#34;&gt;ML for Healthcare&lt;/h2&gt;
&lt;p&gt;Fei-Fei Li and the Stanford group noted that there are plenty of issues throughout the hospital — not just in pathology and radiology. They proposed some interesting problems and solutions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://aicare.stanford.edu/projects/hand_hygiene/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hand Hygiene&lt;/strong&gt;&lt;/a&gt; — More people dying from hospital caused infection than car crashes. They proposed using a CNN with spatial transformer net along with a tracking system to see peoples movement through the hospital.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://ai.stanford.edu/~syyeung/jin_nips_ml4h_2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Surgery evaluation&lt;/strong&gt;&lt;/a&gt; — Using a CNN, locate and follow the movement of surgeon’s tools. Use unnecessary movements as a proxy for surgeons skill.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Greg Corrado gave a interesting talk about how ML will change medicine:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Diagnostics&lt;/strong&gt; — We know there will be a massive expansion in the use of imaging and the information that we get out of it. The question now is how do doctors and machines collaborate? There is always the problem of doctors not using the technology if it disrupts their workflow. We have lots of advances in pathology and radiology so far, and this will start saving lives very soon.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Care management and decision support&lt;/strong&gt; — Electronic medical records are a mess with many coding systems and inconsistencies, and most data isn’t used by humans or Ml methods. Unstructured data is more difficult than something like radiology, but promising results have been shown. There is a lot of promise in making EHR smart, but this may take decades. Medical records -&amp;gt;fihr resources&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Personalized medicine&lt;/strong&gt; — We are just getting started on this, but there are promising results in genetics and eventually this will be a crucial part of healthcare. Eventually we will use some kind of omics for all routine medical prescription, but this may be a long time away.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;medical-imaging&#34;&gt;Medical Imaging&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We need more &lt;em&gt;&lt;strong&gt;adaptive radiation treatment&lt;/strong&gt;&lt;/em&gt;. Currently patients are moving too freely, and we don’t properly take into account of tumour location over time. We have a good idea how normal tissue responds to radiation, but are uncertain about tumour response, so more accuracy in radiation targeting would allow for higher doses and less side effects. 1% increase in radiation power can increase treatment success probability 1–2%.&lt;/li&gt;
&lt;li&gt;We must focus on &lt;em&gt;&lt;strong&gt;end to end approaches for detection&lt;/strong&gt;&lt;/em&gt;, segmentation, etc. Currently we often have an intermediate step of creating something that is visible to the doctor, but this may be inferior to an end to end model.&lt;/li&gt;
&lt;li&gt;Thanks to 
&lt;a href=&#34;http://www.niftynet.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;niftynet&lt;/strong&gt;&lt;/a&gt;, routine medical imaging analysis will become much easier.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practical-concerns&#34;&gt;Practical Concerns:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Top down (ICU data) vs. Bottom up data (Fitbit)&lt;/strong&gt;&lt;/em&gt;. Physicians traditionally only look at data originating from the hospital, if they look at it at all, but now patients are creating their own data. Physicians are not trained or incentivized to interpret the quantified self data. If the incentives and training are set up correctly this might change.&lt;/li&gt;
&lt;li&gt;We don’t have full data sharing because hospitals compete with each other in the private healthcare system. This was a &lt;strong&gt;US centric discussion&lt;/strong&gt;, and these incentives are completely different in other parts of the world.&lt;/li&gt;
&lt;li&gt;Work in this field has gone from image segmentation and classification to clinical problems. For work to become practically useful, &lt;strong&gt;researchers will need to have clinical collaborators&lt;/strong&gt;. Now we have less purely technical challenges and more about how to introduce methods that fit into doctor’s workflow, and best take advantage of doctor’s and machine learning together.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GAN generated data can’t make up for lack of data&lt;/strong&gt; in the first place, but may be useful for anonymization of sensitive data, so it can still be used to train models. (There was argument over this, but I think augmentation with GAN generated images can’t be better using a model that generalizes well. GANs can’t get you outside of the space of images you’ve seen, so there is no replacement for examples of unusual illness)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Research may not be addressing clinical problems as fast as we think&lt;/strong&gt;. Collaboration between doctors and computer scientists still lower than we would like.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive training&lt;/strong&gt; of models with radiologist’s annotations may be a more efficient way to get training data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Radiologists won’t be out of work&lt;/strong&gt; anytime soon because imaging is already increasing faster than radiologists are being trained. Imaging use could increase even further as we discover ways to extract more information from each image. Radiologists work will change, and they may become overly specialized because all the easy tasks will be done by ai.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Future radiologists may be the data scientists of medicine&lt;/strong&gt;, using their understanding to improve models.&lt;/li&gt;
&lt;li&gt;Translating advances in AI into the hospital is difficult because &lt;strong&gt;improving one task is not the same as improving the whole diagnostic workflow&lt;/strong&gt;. We need to look at the entire workflow in the hospital and see see how tech makes the process better overall.&lt;/li&gt;
&lt;li&gt;Currently hospitals buy static tools. With machine learning we can have &lt;strong&gt;tools that adapt to different problems&lt;/strong&gt;, so there will be a different relationship between vendor and consumer.&lt;/li&gt;
&lt;li&gt;I was surprised at the interest in this field. Most of the main tech companies have an interest in the pile of money that is healthcare. This being said, it will still take years for anything to be implemented.&lt;/li&gt;
&lt;li&gt;Because of the investment in healthcare its important for researchers/startups to not compete directly with with google et al. on segmentation and detection accuracy unless they have new methods. Slight architecture tweaks probably won’t beat millions of labelled images. The room for improvement is in better collaborations and more novel methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fairness&#34;&gt;Fairness&lt;/h2&gt;
&lt;p&gt;Now that we have seen so many successes in machine learning there’s increased interest in promoting fairness in AI. Here are some take aways from the 
&lt;a href=&#34;https://www.facebook.com/nipsfoundation/videos/1553500344741199/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;presentations&lt;/a&gt; on bias in AI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data can be biased either because of a group being &lt;strong&gt;underrepresented&lt;/strong&gt;, or because data can incorporate &lt;strong&gt;previous biases&lt;/strong&gt;. For example, blacks may be predicted to be more likely to be criminals because they were arrested more frequently because of racial profiling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Indirect effects of bias are important too&lt;/strong&gt;. You should think about the effects of what you are doing on people. Not only direct effects like not hiring someone, but if you are creating an unfair representation of individuals that could harm them. For example the google search for ‘CEO’ yields primarily white males and this lack of representation of other groups will make them feel they are not CEO material.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These discussions were useful, but in the end we were left with more questions than answers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are some &lt;strong&gt;fundamental contradictions&lt;/strong&gt; between commonly accepted definitions of fairness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How will we decide what is good?&lt;/strong&gt; Should a search result for CEO only return white males because this is the mode of the population? Should it return 8–10% female because that is the true value? Or should it return 50% female because we decided that is the socially best number?
** There is the trade off of &lt;strong&gt;now vs. future&lt;/strong&gt;. Should we sacrifice some efficiency now, so a marginalized group is better represented in the future?&lt;/li&gt;
&lt;li&gt;And even if we decide what to do, there will be the &lt;strong&gt;conflict between what we consider acceptable and what clients want&lt;/strong&gt;. What to do when our boss suggests something questionable?&lt;/li&gt;
&lt;li&gt;We must ask ourselves &lt;strong&gt;who will benefit and who will be harmed&lt;/strong&gt; by our work?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even for someone who isn’t particularly interested in fairness, it is important to pay attention to this work. Now that these discussions are happening, ignorance will not be an excuse for mistakes like the Google made, and there will be bigger repercussions for someone who harms marginalized groups through machine learning.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;meta-learningreinforcement-learningself-playgeneralization&#34;&gt;Meta Learning/Reinforcement Learning/Self-Play/Generalization&lt;/h2&gt;
&lt;p&gt;I knew nothing of this field before the conference, but this is the topic I found most interesting. I will be cracking open the deep RL book soon, but since that hasn’t happened yet don’t trust anything written here.&lt;/p&gt;
&lt;p&gt;Josh Tenenbaum explained that human learning doesn’t happen at an single level. There are different processes in our brain, operating at different time scales:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Perception (&amp;lt;1 second)&lt;/li&gt;
&lt;li&gt;Thinking (&amp;lt;1min)&lt;/li&gt;
&lt;li&gt;Learning (&amp;gt;1minute)&lt;/li&gt;
&lt;li&gt;Development(lifetime)&lt;/li&gt;
&lt;li&gt;Evolution(many lifetimes)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A lot of work seems to be trying take the idea that we learn in different ways in different time scales into account in order to make models generalize better or train faster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;We don’t relearn physics with every task we do&lt;/strong&gt;, so RL methods shouldn’t have to either. Current RL must learn things that we should take as given like physics. Our world is set up with a specific physics, so there is a huge search space of alternatives we don’t need to look at. Solve by using explicit models using probabilistic programming. (Josh Tenenbaum)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Meta Learning&lt;/strong&gt; — Our reinforcement learning algorithms are extremity inefficient compared to how people learn. We should learn the algorithm by training across multiple environments and see which generalize better. Also we can use experience replay to deal with the issue of lack of rewards in RL as shown as Yann Le Cun’s cake (Pieter Abbeel).&lt;/li&gt;
&lt;li&gt;Schmidhuber reminded everyone that their definition of meta learning was incorrect, and that he had been &lt;strong&gt;doing it since 1990&lt;/strong&gt;. He made it clear that anything resembling transfer learning is not meta learning. Learned checkpointing systems for models, models inventing their own problems, and slow-fast networks were some of what he contributed. I imagine there would be a lot of value in going back to his older papers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimal reward&lt;/strong&gt; — Currently we confound our own preferences, and the loss the agent optimizes. Alternatively we can think of finding an internal reward function that is optimal to train the agent.
Capsules — I’m not sure how this fits in with the rest, but it is an attempt to make visual recognition generalize better by having a more restrictive architecture than normal CNNs, making them learn descriptions of objects. This should be closer to the human visual system, and work better with changes in viewpoint and small data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self-Play&lt;/strong&gt; — Allows use to create complex strategies from simple environments by having agents compete. This is a way to convert compute into data, and as the game goes on we learn more valuable things. Problem is making sure these games don’t get stuck at very basic equilibria. This is how AlphaZero dominated go and chess. IIya Sutskever seemed very confident this is the way forward to create better AI.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;random-stuff&#34;&gt;Random Stuff:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;“To get an extra 5% to bold the number in your table, you change the architecture. This is to make a model easier to optimize, not make it more flexible.” — I forget who said this, but this is a good reminder of how we can’t think about deep learning in the same way as more simple models.&lt;/li&gt;
&lt;li&gt;In the context on interpretability, we should focus on outliers, as these can give us the most information. — I forget who said this.&lt;/li&gt;
&lt;li&gt;Schmidhuber’s ways are extremely off-putting, but I imagine I’d be pretty pissed off too if I had not been recognized for my work to the extent he has. Maybe I’m wrong, but his group seems to have done a lot of good work in the past&lt;/li&gt;
&lt;li&gt;Andrej Karpathy, IIya Sutskever and Elon Musk seem quite confident in the speed we will see AGI sooner than we expect. Normally I would dismiss people who say this, but these people are the few who know something most of us don’t.&lt;/li&gt;
&lt;li&gt;China will dominate machine learning driven healthcare because of their huge amounts of data and lack of privacy concerns.&lt;/li&gt;
&lt;li&gt;Genetics, Bayesian Deep learning, GANs, Art Generation and many other topics received plenty attention at this conference, just not from me.&lt;/li&gt;
&lt;li&gt;There were of non-tech firms like hedge funds, but less interest in this from the participants. Also I was surprised at the number of Chinese firms trying to grab up the best ml researchers.&lt;/li&gt;
&lt;li&gt;Nvidia, Intel, and even Tesla are making new DL chips. Time to end this monopoly and obscene prices.&lt;/li&gt;
&lt;li&gt;Tencent had the best free t-shirt. Quality stuff.&lt;/li&gt;
&lt;li&gt;The hype here is insane. Will we have the type of bubble we had in 1999 and the corresponding crash, even though the underlying technology was correct? We know machine learning will be the way forward, but real applications may take a while to get sorted out, and there could be plenty of time in between for another little AI winter.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://renebidart.com/post/2020-01-11-hk/2020-01-11-what-i-learned-in-hong-kong/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://renebidart.com/post/2020-01-11-hk/2020-01-11-what-i-learned-in-hong-kong/</guid>
      <description>&lt;h2 id=&#34;title-what-i-learned-in-hong-kong&#34;&gt;neyman-paper&amp;mdash;
author: Rene Bidart
date: &amp;ldquo;2020-01-11T00:00:00Z&amp;rdquo;
title: What I Learned in Hong Kong&lt;/h2&gt;
&lt;h2 id=&#34;香港被和谐&#34;&gt;香港被和谐&lt;/h2&gt;
&lt;p&gt;I spent the last few months interning at Shell Street Labs in Hong Kong (September-December 2020), and this opened my eyes to the mess that is media, propaganda, and group identity.&lt;/p&gt;
&lt;h3 id=&#34;media-bullshit&#34;&gt;Media Bullshit&lt;/h3&gt;
&lt;p&gt;I made it a point to follow some Hong Kong, Western, and CCP news sources on twitter. The news was divided over if the police or the protesters caused of most of the violence, but it was all sensationalist. The one thing they were in agreement about was that Hong Kong and the protests were dangerous, but this was the only thing that wasn&amp;rsquo;t true.&lt;/p&gt;
&lt;p&gt;I expected the protests to be filled with violent people ready for war, like what was shown in the news. The reality was totally different, it was mostly innocent innocent, skinny university age kids, with a few elderly people and children. There were so many couples holding hands walking through the protests, I couldn&amp;rsquo;t believe what was going on. This didn&amp;rsquo;t look like they were capable of anything like I&amp;rsquo;d seen in the news. Most people ran from tear gas, not towards it.&lt;/p&gt;
&lt;p&gt;The violence and destruction shown on the news did happen, but it was a small groups of people at the end of a sea of protesters. On the news the most extreme events are repeated over and over until another crazy thing happens, that can replaces. At the end of the day Hong Kong is still much less dangerous than a place like Detroit, no matter how much more media coverage it gets.&lt;/p&gt;
&lt;p&gt;Both the protesters and the CCP media were promoting bullshit. I expected more from the CCP, but really both sides continually distort reality to convince you of their side of the story. The desire to maximise views can be as misleading as intentional government propaganda. Now I have less trust in anything I see. I always questioned things for accuracy, but now I the focus is more on how the truth is distorted rather than outright fake news.&lt;/p&gt;
&lt;h3 id=&#34;nothing-brings-people-together-more-than-a-common-enemy&#34;&gt;Nothing brings people together more than a common enemy&lt;/h3&gt;
&lt;p&gt;At first I couldn&amp;rsquo;t understand why so many people would be willing to protest week after week, wasting their weekends in the streets? There was something more to this, and it wasn&amp;rsquo;t until I actually saw the protest that I understood.&lt;/p&gt;
&lt;p&gt;Even as an outsider looking in seeing these protests gives you a strange feeling. All these masked people who barely knew each other had a connection I&amp;rsquo;ve never seen before, it felt like this was the most united I&amp;rsquo;ve ever seen a group of people. Having an enemy unites people like nothing else, and I&amp;rsquo;ve noticed more and more this is the backbone of propaganda across the world.&lt;/p&gt;
&lt;p&gt;How much does having a clear enemy explain the recent success of China and the current dysfunction in the west? Would there be so much partisan bullshit if we feared China as much as they fear us? Would there be so much pressure to attack each other over pronouns and definitions of feminism if we had some real danger? I don&amp;rsquo;t know the answer, but realising that something so horrible is what unites us best wasn&amp;rsquo;t a good feeling.&lt;/p&gt;
&lt;p&gt;I think there&amp;rsquo;s another reason protesting became popular, and that&amp;rsquo;s that it became cool. Protesting is a high status activity for young people, and social media makes it possible for everyone to know you&amp;rsquo;re protesting, so you&amp;rsquo;re more than another masked person in a crowd. There&amp;rsquo;s protest art all over social media, and even plenty of Tinder profiles saying &amp;ldquo;No cops&amp;rdquo; or &amp;ldquo;香港人加油&amp;rdquo;.&lt;/p&gt;
&lt;h3 id=&#34;division&#34;&gt;Division&lt;/h3&gt;
&lt;p&gt;Why is it that people who agree on 99% of things and are good friends would be willing to kill each other for some abstract idea like nationalism or freedom? I met multiple people who had their relationships with coworkers and family change when this religious disagreement was brought into the forefront. I knew in abstract that this could happen, but seeing this in reality was very uncomfortable.&lt;/p&gt;
&lt;p&gt;It seemed especially strange for mainland Chinese people, who after living in Hong Kong for many years and even learning the Cantonese language to considered the outsider or even the enemy - someone who was brought here by the CCP to help conquer Hong Kong. Are they only immigrants, or are they settlers like the British were, bringing their own culture and their own language on behalf of the CCP? Modern day colonialists forcing a single definition of what it means to be Chinese? Why must the Mainland Chinese impose their will on the Hong Kong people?&lt;/p&gt;
&lt;p&gt;Or are the protestors the problem, being childish and demanding their own special treatment from China? Or even traitors for preferring Western style governance after all the success of China? Even supporting violence against innocent people to achieve these pointless goals?&lt;/p&gt;
&lt;p&gt;Everyone accuses the other side of being brainwashed. The Hong Kong people are being brainwashed by not having a good nationalist education, and the mainland Chinese people are because of it. But why is it so hard to see the other side?&lt;/p&gt;
&lt;p&gt;I think we&amp;rsquo;re designed to not be open minded. Why do Chinese people will reflexively mention how much better Shenzhen is than Hong Kong whenever Shenzhen is brought up? Or if you mentioned Cantonese is a different language then Mandarin, you’re quickly corrected that it’s a dialect, not a language. How do they come up with this strangely pedantic statement so consistently?&lt;/p&gt;
&lt;p&gt;At first I thought there was something special about their government&amp;rsquo;s propaganda, that they will not only make someone believe something, but also to consistently be vocal about it. But this isn&amp;rsquo;t right, this is something that happens to us all.&lt;/p&gt;
&lt;p&gt;I think we have a natural defence mechanism to preserve our religion, so whenever we notice a threat (consciously or unconsciously), we attack it. It is similar to why people are so quick to insult Trump, or anything that reminds them people have different opinions. Chinese people on some level know that a&amp;rsquo; preference for Hong Kong over carefully designed Shenzhen in spite of all its problems is an attack on their government, the same way the fact the existence of Trump as president is a constant attack on progressive views.&lt;/p&gt;
&lt;h3 id=&#34;hope&#34;&gt;Hope?&lt;/h3&gt;
&lt;p&gt;I can&amp;rsquo;t help but feel bad for everyone stuck in this situation, it seems like the people of Hong Kong are just pawns for more powerful people. Foreigners use it as their capitalist paradise, either wanting protesters killed for disrupting their way to work / LKF, or siding with protesters but unwilling to do anything to hinder their chance of work in mainland China. The mainland Chinese view it as something to be conquered to preserve the harmony of their great country. The CIA sees this as way to divide China. The Hong Kong government only cares about the richest people and the CCP, resulting in the housing crisis and continued protests.&lt;/p&gt;
&lt;p&gt;Is there a solution?  My guess is no, unless the people of Hong Kong embrace Chinese nationalism they will be left for a slow decline by the CCP. This is unlikely, because there has been a decline in Hong Kong people identifying as Chinese since 2008, which is around the time China increased its nationalism, deciding that there was a single way to be Chinese which was not going to include the current culture of Hong Kong.&lt;/p&gt;
&lt;p&gt;By creating Shenzhen, the CCP has shown that the China can be superior economically to the West, and after seeing this the people of Hong Kong were supposed to see the glory of China. But so many people in Hong Kong are unwilling to give up their values of self determination and freedom, which are totally unacceptable for China. The best way forward for the CCP is to encourage incompetent governments like Carrie Lam that are slowly destroying Hong Kong. Chinese news sources will continue to show the disaster this has caused, both economically and for social harmony, strengthening the resolve of Chinese people that their form of government is best.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
